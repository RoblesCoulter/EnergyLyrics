{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from glove import Corpus, Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(word_count, emotional_mapping):\n",
    "    # full = generate_IEMOCAP_df()\n",
    "    data = pd.read_csv('data/IEMOCAP_sentences.csv',index_col=0)\n",
    "    data['emotion_code'] = data['emotion'].map( emotional_mapping ).astype(int)\n",
    "    # Take away fear, surprise,disgust, xxx and others. Not enough data\n",
    "    data = data[data.emotion_code < 6]\n",
    "    # Clean Transcripts\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "    # Filter Word Count\n",
    "    data = filter_word_count(data, word_count)\n",
    "#     data,patterns = remove_empty_patterns(data,patterns)\n",
    "    return data\n",
    "\n",
    "def clean_text(text):\n",
    "    punct_str = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~«»“…‘”\\t'\n",
    "    for p in punct_str:\n",
    "        text = text.replace(p,' ')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    text = re.sub(\".*?\\[(.*?)\\]\",\"\",text) # Take out any [action] text in the transcript\n",
    "    return text.lower().strip()\n",
    "\n",
    "def filter_word_count(data, n_count):\n",
    "    return data[list(map(lambda x: len(x.split(' ')) >= n_count,data['text']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is there a problem</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>well what's the problem let me change it</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>what i'm getting an id this is why i'm here my...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>how am i supposed to get an id without an id h...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i'm here to get an id</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  emotion_code\n",
       "2                                 is there a problem             3\n",
       "5           well what's the problem let me change it             3\n",
       "6  what i'm getting an id this is why i'm here my...             4\n",
       "7  how am i supposed to get an id without an id h...             4\n",
       "8                              i'm here to get an id             4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotional_mapping = {'ang': 0, 'sad': 1, 'exc': 2, 'neu': 3,'fru': 4,'hap': 5,'fea': 6,'sur': 7,'dis': 8, 'xxx':9,'oth':10}\n",
    "data = load_data(3, emotional_mapping)\n",
    "df = data[['text','emotion_code']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'glove.glove' from '/Users/roblescoulter/anaconda3/lib/python3.6/site-packages/glove/glove.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "TEST_SIZE      = 0.2\n",
    "\n",
    "# EMBEDDING\n",
    "MAX_NUM_WORDS  = 2500 # 2954, 2000, 2700\n",
    "EMBEDDING_DIM  = 200\n",
    "MAX_SEQ_LENGTH = 100\n",
    "USE_GLOVE      = True\n",
    "\n",
    "# MODEL\n",
    "FILTER_SIZES   = [3,4,5]\n",
    "FEATURE_MAPS   = [10,10,10]\n",
    "DROPOUT_RATE   = 0.5\n",
    "\n",
    "# LEARNING\n",
    "BATCH_SIZE     = 200\n",
    "NB_EPOCHS      = 40\n",
    "RUNS           = 5\n",
    "VAL_SIZE       = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data.text, data.emotion_code, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text informations:\n",
      "max length: 100 / min length: 3 / mean length: 13 / limit length: 100\n",
      "vocabulary size: 2930 / limit: 2500\n"
     ]
    }
   ],
   "source": [
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    "\n",
    "tokenizer = Tokenizer()#num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "length = max_length(x_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "result = [len(x.split()) for x in x_train]\n",
    "print('Text informations:')\n",
    "print('max length: %i / min length: %i / mean length: %i / limit length: %i' % (np.max(result),\n",
    "                                                                                np.min(result),\n",
    "                                                                                np.mean(result),\n",
    "                                                                                MAX_SEQ_LENGTH))\n",
    "\n",
    "print('vocabulary size: %i / limit: %i' % (len(word_index), MAX_NUM_WORDS))\n",
    "\n",
    "# Padding all sequences to same length of `MAX_SEQ_LENGTH`\n",
    "data   = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_embeddings(data, use_text8 = False):\n",
    "    sentences = [x.split() for x in data]\n",
    "    print(sentences)\n",
    "    \n",
    "def create_word2vec_embeddings(data = None, use_text8 = False):\n",
    "    model = None\n",
    "    if(use_text8):\n",
    "        model = KeyedVectors.load_word2vec_format('models/text8.model.bin',binary=True)\n",
    "    else:\n",
    "        if(data != None):  \n",
    "            model = word2vec.Word2Vec(data, size=EMBEDDING_DIM)\n",
    "        else:\n",
    "            print('No data found. Using text8 Corpus')\n",
    "            model = KeyedVectors.load_word2vec_format('models/text8.model.bin',binary=True)\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    for word in model.wv.index2word:\n",
    "        embeddings_index[word] = model.wv[word]\n",
    "        \n",
    "    embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if(embedding_vector is not None):\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return Embedding(input_dim=MAX_NUM_WORDS, output_dim=EMBEDDING_DIM, input_length = MAX_SEQ_LENGTH,\n",
    "                    weights= [embedding_matrix], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('so', 0.9997955560684204),\n",
       " (\"'cause\", 0.9997822046279907),\n",
       " ('um', 0.9997684955596924),\n",
       " ('hear', 0.9997516870498657),\n",
       " ('enough', 0.9997501373291016),\n",
       " ('live', 0.9997497797012329),\n",
       " ('though', 0.999740481376648),\n",
       " ('because', 0.9997397065162659),\n",
       " ('augie', 0.9997377991676331),\n",
       " ('stupid', 0.9997369647026062)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings = create_word2vec_embeddings(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    }
   ],
   "source": [
    "sentences = list(itertools.islice(word2vec.Text8Corpus('data/text8'),None))\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=10)\n",
    "glove = Glove(no_components=100,learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30,no_threads=4,verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "# model = word2vec.Word2Vec(sentences, size=EMBEDDING_DIM)\n",
    "# model.save('models/text8.model')\n",
    "# model.wv.save_word2vec_format('models/text8.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = KeyedVectors.load_word2vec_format('models/text8.model.bin',binary=True)\n",
    "# # model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
