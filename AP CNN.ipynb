{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Pickling\n",
    "from six.moves import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "no_alignment_file = [4764]\n",
    "wrong_alignment = [3730]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from glove import Corpus, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_patterns(data,extract=False):\n",
    "    if(extract):\n",
    "        patterns = {}\n",
    "        for index, row in data.iterrows():\n",
    "            patterns[row['index']] = set(get_pattern([row['text']])[0].values())\n",
    "            print('Extracted pattern from '+ row['index'] + ' index:'+ str(index))\n",
    "            print('Size: ', len(patterns[row['index']]), 'Patterns size', len(patterns))\n",
    "        try:\n",
    "            print('Saving Pickle')\n",
    "            with open('pickles/patterns/pattern.pickle','wb') as f:\n",
    "                save = {\n",
    "                    'patterns' : patterns\n",
    "                }\n",
    "                pickle.dump(save,f,pickle.HIGHEST_PROTOCOL)\n",
    "                print('Successfully saved in pattern.pickle')\n",
    "                return patterns\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to pickle', e)\n",
    "            print('Patterns probably not saved.')\n",
    "            return patterns\n",
    "    else:\n",
    "        try:\n",
    "            with open('pickles/patterns/pattern.pickle','rb') as f:\n",
    "                save = pickle.load(f)\n",
    "                patterns = save['patterns']\n",
    "                del save\n",
    "                returning = {}\n",
    "                for key in list(data['index']):\n",
    "                    returning[key] = patterns[key]\n",
    "                return returning\n",
    "        except Exception as e:\n",
    "            print('Error loading base datasets pickle: ', e)\n",
    "            \n",
    "def clean_text(text):\n",
    "    punct_str = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~«»“…‘”\\t'\n",
    "    for p in punct_str:\n",
    "        text = text.replace(p,' ')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def filter_word_count(data, n_count):\n",
    "    return data[list(map(lambda x: len(x.split(' ')) >= n_count,data['text']))]\n",
    "\n",
    "def remove_empty_patterns(data,patterns):\n",
    "    empty_patterns = [k for k, v in patterns.items() if len(v) < 1]\n",
    "    patterns = { k:v for k, v in patterns.items() if len(v) >= 1 }\n",
    "    data = filter(lambda x: x[1]['index'] not in empty_patterns ,data.iterrows())\n",
    "    data = pd.DataFrame.from_items(data).T\n",
    "    return data,patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(word_count,emotional_mapping):\n",
    "    # full = generate_IEMOCAP_df()\n",
    "    data = pd.read_csv('data/IEMOCAP_sentences_votebased.csv',index_col=0)\n",
    "    data['emotion_code'] = data['emotion'].map( emotional_mapping ).astype(int)\n",
    "    # Take away fear, surprise,disgust, xxx and others. Not enough data\n",
    "    data = data[data.emotion_code < 4]\n",
    "    #Remove rows that don't have Alignment file\n",
    "#     data = data.drop(no_alignment_file)\n",
    "    # Remove rows that have wrong Alignment file\n",
    "    data = data.drop(wrong_alignment)\n",
    "    # Clean Transcripts\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "    # Filter Word Count\n",
    "    data = filter_word_count(data, word_count)\n",
    "    patterns = extract_patterns(data)\n",
    "    data,patterns = remove_empty_patterns(data,patterns)\n",
    "    return data,patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>text</th>\n",
       "      <th>wav_path</th>\n",
       "      <th>alignment_path</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "      <th>gender</th>\n",
       "      <th>emotion_code</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ang</th>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "      <td>1153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hap</th>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neu</th>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "      <td>1466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sad</th>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "      <td>974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  start_time  end_time  text  wav_path  alignment_path  valence  \\\n",
       "emotion                                                                         \n",
       "ang       1153        1153      1153  1153      1153            1153     1153   \n",
       "hap        689         689       689   689       689             689      689   \n",
       "neu       1466        1466      1466  1466      1466            1466     1466   \n",
       "sad        974         974       974   974       974             974      974   \n",
       "\n",
       "         arousal  dominance  gender  emotion_code  \n",
       "emotion                                            \n",
       "ang         1153       1153    1153          1153  \n",
       "hap          689        689     689           689  \n",
       "neu         1466       1466    1466          1466  \n",
       "sad          974        974     974           974  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotional_mapping = {'ang': 0, 'sad': 1, 'hap': 2, 'neu': 3,'fru': 4,'exc': 5,'fea': 6,'sur': 7,'dis': 8, 'xxx':9,'oth':10}\n",
    "data, patterns = load_data(3,emotional_mapping)\n",
    "data.groupby('emotion').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_feature_table = None\n",
    "try:\n",
    "    with open('pickles/patterns/scaled_pattern_features4emo.pickle','rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        scaled_feature_table = save['feature_table']\n",
    "        del save\n",
    "except Exception as e:\n",
    "    print('Error loading pattern features pickle: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'25086_.+ enough': array([ 1.47662568,  1.64670445,  0.19012376, -0.06221796,  0.41093638,\n",
       "         0.83077829,  0.24317282,  0.63061836, -0.42022054,  0.11148869,\n",
       "         1.83209683,  1.47504898,  1.56258218, -0.23396023,  0.31952995,\n",
       "         0.23395897,  0.37763398,  0.72121466, -0.72438772,  0.41479379,\n",
       "         1.36804605,  0.02490234,  0.06434849,  0.08740234])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = scaled_feature_table['Ses01F_impro03_M024']\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATASET\n",
    "TEST_SIZE      = 0.2\n",
    "\n",
    "# EMBEDDING\n",
    "# MAX_NUM_WORDS  = 1800 \n",
    "EMBEDDING_DIM  = 24\n",
    "MAX_SEQ_LENGTH = 500\n",
    "USE_GLOVE      = True\n",
    "\n",
    "# MODEL\n",
    "FILTER_SIZES   = [2,2,3]\n",
    "FEATURE_MAPS   = [100,100,100]\n",
    "DROPOUT_RATE   = 0.5\n",
    "\n",
    "# LEARNING\n",
    "BATCH_SIZE     = 200\n",
    "NB_EPOCHS      = 50\n",
    "RUNS           = 5\n",
    "VAL_SIZE       = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, data.emotion_code, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text informations:\n",
      "max length: 491 / min length: 1 / mean length: 43 / limit length: 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "# tokenizer.fit_on_texts(x_train)\n",
    "# sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "result = [len(scaled_feature_table[x].keys()) for x in x_train['index']]\n",
    "# result = [len(scaled_feature_table[x].keys()) for x in scaled_feature_table.keys()]\n",
    "\n",
    "print('Text informations:')\n",
    "print('max length: %i / min length: %i / mean length: %i / limit length: %i' % (np.max(result),\n",
    "                                                                                np.min(result),\n",
    "                                                                                np.mean(result),\n",
    "                                                                                MAX_SEQ_LENGTH))\n",
    "\n",
    "# print('vocabulary size: %i / limit: %i' % (len(word_index), MAX_NUM_WORDS))\n",
    "data = []\n",
    "for key, row in x_train.iterrows():\n",
    "    features = scaled_feature_table[row['index']]\n",
    "    sorted(features)\n",
    "#     sequences.append(features.values())\n",
    "    feat_matrix = np.array(features.values())\n",
    "    pad = np.zeros((MAX_SEQ_LENGTH,EMBEDDING_DIM))\n",
    "    pad[:feat_matrix.shape[0],:feat_matrix.shape[1]] = feat_matrix\n",
    "    data.append(pad)\n",
    "    \n",
    "# # Padding all sequences to same length of `MAX_SEQ_LENGTH`\n",
    "# data   = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 1/5\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Please define `num_words` and `embedding_dim` if you not use a pre-trained embedding",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-36dce2a63fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfeature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFEATURE_MAPS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDROPOUT_RATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     )\n",
      "\u001b[0;32m/home/luisrobles/Dev/Thesis/cnn_model.pyc\u001b[0m in \u001b[0;36mbuild_cnn\u001b[0;34m(embedding_layer, num_words, embedding_dim, filter_sizes, feature_maps, max_seq_length, dropout_rate)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please define `filter_sizes` and `feature_maps` with the same length.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0membedding_layer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please define `num_words` and `embedding_dim` if you not use a pre-trained embedding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating CNN %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Please define `num_words` and `embedding_dim` if you not use a pre-trained embedding"
     ]
    }
   ],
   "source": [
    "import cnn_model\n",
    "\n",
    "histories = []\n",
    "import time\n",
    "for i in range(RUNS):\n",
    "    print('Running iteration %i/%i' % (i+1, RUNS))\n",
    "    start_time = time.time()\n",
    "    X_train, X_val, labels, y_val = train_test_split(data, y_train, test_size=VAL_SIZE, random_state=42)\n",
    "    \n",
    "    emb_layer = None\n",
    "    \n",
    "    model = cnn_model.build_cnn(\n",
    "        embedding_layer=emb_layer,\n",
    "        embedding_dim= EMBEDDING_DIM,\n",
    "        filter_sizes = FILTER_SIZES,\n",
    "        feature_maps = FEATURE_MAPS,\n",
    "        max_seq_length = MAX_SEQ_LENGTH,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
