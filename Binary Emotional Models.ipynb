{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "#Data handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models \n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Pickling\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import average_precision_score, recall_score, precision_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as ms\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import seaborn\n",
    "ms.use('seaborn-muted')\n",
    "%matplotlib inline\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gender(text):\n",
    "    text = text[len(text)-4]\n",
    "    if('F' in text):\n",
    "        return 'F'\n",
    "    elif('M' in text):\n",
    "        return 'M'\n",
    "    else:\n",
    "        return 'N/A'\n",
    "    \n",
    "def get_word_segments(align_file):\n",
    "    segments = []\n",
    "    with open(align_file,'r') as openFile:\n",
    "        read_lines = openFile.readlines()\n",
    "        lines = read_lines[1:len(read_lines)-1]\n",
    "        for line in lines:\n",
    "            splitted = line.split()\n",
    "            segments.append({'sf': splitted[0], 'ef':splitted[1],'seg_scr':splitted[2],'word':splitted[3]})\n",
    "    return segments\n",
    "    \n",
    "def generate_IEMOCAP_df():\n",
    "    root_path = 'data/IEMOCAP_full_release/'\n",
    "    folders = os.listdir(root_path)\n",
    "    session_folders = filter(lambda x: x.startswith('Session'),folders)\n",
    "    sentences_wav = '/sentences/wav/'\n",
    "    forced_alignment = '/sentences/ForcedAlignment/'\n",
    "    dialog_transcriptions = '/dialog/transcriptions/' #Ses01F_impro01.txt\n",
    "    dialog_emo = '/dialog/EmoEvaluation/' #Ses01F_impro01.txt\n",
    "    data = {}\n",
    "    emotions = {}\n",
    "    alignment = {}\n",
    "    for session in session_folders:\n",
    "        temp_root = root_path + session\n",
    "        temp_conversations_path = temp_root + sentences_wav\n",
    "        temp_transcripts_path = temp_root + dialog_transcriptions\n",
    "        temp_alignments_path = temp_root + forced_alignment\n",
    "        temp_emo_path = temp_root + dialog_emo\n",
    "        emo_eval = os.listdir(temp_emo_path)\n",
    "        for emo_file in emo_eval:\n",
    "            emo_file_path = temp_emo_path + emo_file\n",
    "            if(emo_file.endswith('.txt')):\n",
    "                with open(emo_file_path,'r') as openFile:\n",
    "                    for line in openFile.readlines():\n",
    "                        if(line.startswith('[')):\n",
    "                            line = line.split()\n",
    "                            key = line[3]\n",
    "                            emotion = line[4]\n",
    "                            valence = re.search('[0-9]+.[0-9]+', line[5]).group(0)\n",
    "                            arousal = re.search('[0-9]+.[0-9]+',line[6]).group(0)\n",
    "                            dominance = re.search('[0-9]+.[0-9]+',line[7]).group(0)\n",
    "                            emotions[key] = [emotion,valence,arousal, dominance]\n",
    "        transcriptions = os.listdir(temp_transcripts_path)\n",
    "        for transcript in transcriptions:\n",
    "            transcript_path = temp_transcripts_path + transcript\n",
    "            with open(transcript_path,'r') as openFile:\n",
    "                for line in openFile.readlines():\n",
    "                    if(line.startswith('Ses')):\n",
    "                        line = line.split(' [')\n",
    "                        key = line[0]\n",
    "                        if('XX' not in key):\n",
    "                            line = ' ['.join(line[1:])\n",
    "                            line = line.split(']: ')\n",
    "                            time = line[0]\n",
    "                            script = ']: '.join(line[1:])\n",
    "                            start_t, end_t = time.split('-')\n",
    "                            data[key] = [start_t,end_t,script.split('\\r\\n')[0]]\n",
    "        conversations = os.listdir(temp_conversations_path)\n",
    "        for conversation in conversations:\n",
    "            files_path = temp_conversations_path+ conversation\n",
    "            alignments_path = temp_alignments_path + conversation\n",
    "            files = os.listdir(files_path)\n",
    "            for file in files:\n",
    "                key,suf = file.split('.')\n",
    "                if(suf == 'wav'):\n",
    "                    data[key].append(files_path + '/'+ file)\n",
    "                    data[key].append(alignments_path + '/'+key+'.wdseg')\n",
    "    sentence_df = pd.DataFrame(data,index=['start_time','end_time','text','wav_path','alignment_path']).T\n",
    "    emotion_df = pd.DataFrame(emotions, index=['emotion','valence','arousal','dominance']).T\n",
    "    full = sentence_df.join(emotion_df)\n",
    "    full = full.reset_index()\n",
    "    full['gender'] = full['index'].apply(get_gender)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_reg(x_train,y_train,x_test,y_test, delete = True):\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(x_train, y_train)\n",
    "    Y_pred = logreg.predict(x_test)\n",
    "    acc_log = round(logreg.score(x_test, y_test) * 100, 2)\n",
    "    precision = precision_score(y_test, Y_pred)\n",
    "    recall = recall_score(y_test,Y_pred)\n",
    "    if(delete):\n",
    "        del logreg\n",
    "        return Y_pred, acc_log, precision, recall\n",
    "    return logreg,Y_pred,acc_log, precision, recall\n",
    "\n",
    "# _, acc_log = log_reg(x_train,ang_train, x_test, ang_test)\n",
    "# print(acc_log)\n",
    "\n",
    "def svc(x_train,y_train,x_test,y_test, delete = True):\n",
    "    svc = SVC()\n",
    "    svc.fit(x_train,y_train)\n",
    "    Y_svcpred = svc.predict(x_test)\n",
    "    acc_svc = round(svc.score(x_test, y_test) * 100,2)\n",
    "    precision = precision_score(y_test, Y_svcpred)\n",
    "    recall = recall_score(y_test,Y_svcpred)\n",
    "    if(delete):\n",
    "        del svc \n",
    "        return Y_svcpred, acc_svc,precision, recall\n",
    "    return svc, Y_svcpred, acc_svc,precision,recall\n",
    "\n",
    "# _, acc_svc = svc(x_train,ang_train,x_test,ang_test)\n",
    "# print(acc_svc)\n",
    "\n",
    "def knn(x_train,y_train,x_test,y_test,n_neighbors = 3,delete = True):\n",
    "    knn = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "    knn.fit(x_train, y_train)\n",
    "    Y_knnpred = knn.predict(x_test)\n",
    "    acc_knn = round(knn.score(x_test, y_test) * 100, 2)\n",
    "    precision = precision_score(y_test, Y_knnpred)\n",
    "    recall = recall_score(y_test,Y_knnpred)\n",
    "    if(delete):\n",
    "        del knn\n",
    "        return Y_knnpred, acc_knn,precision,recall\n",
    "    return knn, Y_knnpred, acc_knn, precision,recall\n",
    "# _,acc_knn = knn(x_train,ang_train,x_test,ang_test)\n",
    "# print(acc_knn)\n",
    "\n",
    "def gaussian(x_train,y_train,x_test,y_test,delete = True):\n",
    "    gaussian = GaussianNB()\n",
    "    gaussian.fit(x_train, y_train)\n",
    "    Y_gaussianpred = gaussian.predict(x_test)\n",
    "    acc_gaussian = round(gaussian.score(x_test, y_test) * 100, 2)\n",
    "    precision = precision_score(y_test, Y_gaussianpred)\n",
    "    recall = recall_score(y_test,Y_gaussianpred)\n",
    "    if(delete):\n",
    "        del gaussian\n",
    "        return Y_gaussianpred, acc_gaussian, precision,recall\n",
    "    return gaussian, Y_gaussianpred, acc_gaussian,precision,recall\n",
    "\n",
    "# _, acc_gaussian = gaussian(x_train,ang_train,x_test,ang_test)\n",
    "# print(acc_gaussian)\n",
    "\n",
    "def perceptron(x_train,y_train,x_test,y_test,delete = True):\n",
    "    perceptron = Perceptron()\n",
    "    perceptron.fit(x_train, y_train)\n",
    "    Y_perceptronpred = perceptron.predict(x_test)\n",
    "    acc_perceptron = round(perceptron.score(x_test, y_test) * 100, 2)\n",
    "    precision = precision_score(y_test, Y_perceptronpred)\n",
    "    recall = recall_score(y_test,Y_perceptronpred)\n",
    "    if(delete):\n",
    "        del perceptron\n",
    "        return Y_perceptronpred, acc_perceptron,precision,recall\n",
    "    return perceptron, Y_perceptronpred ,acc_perceptron,precision,recall\n",
    "# _, acc_perceptron = perceptron(x_train,ang_train,x_test,ang_test)\n",
    "# print(acc_perceptron)\n",
    "\n",
    "def linear_svc(x_train,y_train,x_test,y_test,delete = True):\n",
    "    linear_svc = LinearSVC()\n",
    "    linear_svc.fit(x_train, y_train)\n",
    "    Y_linearsvcpred = linear_svc.predict(x_test)\n",
    "    acc_linear_svc = round(linear_svc.score(x_test, y_test) * 100, 2)\n",
    "    precision = precision_score(y_test, Y_linearsvcpred)\n",
    "    recall = recall_score(y_test,Y_linearsvcpred)\n",
    "    if(delete):\n",
    "        del linear_svc\n",
    "        return Y_linearsvcpred, acc_linear_svc,precision,recall\n",
    "    return linear_svc, Y_linearsvcpred, acc_linear_svc,precision,recall\n",
    "# _, acc_linear_svc = linear_svc(x_train,ang_train,x_test,ang_test)\n",
    "# print(acc_linear_svc)\n",
    "\n",
    "def sgd(x_train,y_train,x_test,y_test,delete = True):\n",
    "    sgd = SGDClassifier()\n",
    "    sgd.fit(x_train, y_train)\n",
    "    Y_linearsgdpred = sgd.predict(x_test)\n",
    "    acc_sgd = round(sgd.score(x_test, y_test) * 100, 2)\n",
    "    precision = precision_score(y_test, Y_linearsgdpred)\n",
    "    recall = recall_score(y_test,Y_linearsgdpred)\n",
    "    if(delete):\n",
    "        del sgd\n",
    "        return Y_linearsgdpred , acc_sgd,precision,recall\n",
    "    return sgd, Y_linearsgdpred, acc_sgd,precision,recall\n",
    "# _, acc_sgd = sgd(x_train,ang_train,x_test,ang_test)\n",
    "# print(acc_sgd)\n",
    "\n",
    "def decision_tree(x_train,y_train,x_test,y_test,delete = True):\n",
    "    decision_tree = DecisionTreeClassifier()\n",
    "    decision_tree.fit(x_train, y_train)\n",
    "    Y_dectreepred = decision_tree.predict(x_test)\n",
    "    acc_decision_tree = round(decision_tree.score(x_test, y_test) * 100, 2)\n",
    "    precision = precision_score(y_test, Y_dectreepred)\n",
    "    recall = recall_score(y_test,Y_dectreepred)\n",
    "    if(delete):\n",
    "        del decision_tree\n",
    "        return Y_dectreepred , acc_decision_tree,precision,recall\n",
    "    return decision_tree, Y_dectreepred , acc_decision_tree,precision,recall\n",
    "# _, acc_decision_tree = decision_tree(x_train,ang_train,x_test,ang_test)\n",
    "# print(acc_decision_tree)\n",
    "\n",
    "def random_forest(x_train,y_train,x_test,y_test,n_estimators = 100, delete = True):\n",
    "    random_forest = RandomForestClassifier(n_estimators= n_estimators)\n",
    "    random_forest.fit(x_train, y_train)\n",
    "    Y_pred = random_forest.predict(x_test)\n",
    "    acc_random_forest = round(random_forest.score(x_test, y_test) * 100, 2)\n",
    "    precision = precision_score(y_test, Y_pred)\n",
    "    recall = recall_score(y_test,Y_pred)\n",
    "    if(delete):\n",
    "        del random_forest\n",
    "        return Y_pred, acc_random_forest,precision,recall\n",
    "    return random_forest, Y_pred, acc_random_forest,precision,recall\n",
    "# _, acc_random_forest = random_forest(x_train,ang_train, x_test , ang_test)\n",
    "# print(acc_random_forest)\n",
    "\n",
    "def compare_models(x_train, y_train, x_test, y_test):\n",
    "    _,acc_svc,pre_svc,rec_svc = svc(x_train,y_train,x_test,y_test)\n",
    "    _,acc_knn,pre_knn,rec_knn = knn(x_train,y_train,x_test,y_test)\n",
    "    _,acc_log,pre_log,rec_log = log_reg(x_train,y_train,x_test,y_test)\n",
    "    _,acc_random_forest,pre_rf,rec_rf = random_forest(x_train,y_train,x_test,y_test)\n",
    "    _,acc_gaussian,pre_gau,rec_gau = gaussian(x_train,y_train,x_test,y_test)\n",
    "    _,acc_perceptron,pre_per,rec_per = perceptron(x_train,y_train,x_test,y_test)\n",
    "    _,acc_sgd,pre_sgd,rec_sgd = sgd(x_train,y_train,x_test,y_test)\n",
    "    _,acc_linear_svc,pre_lsvc,rec_lsvc = linear_svc(x_train,y_train,x_test,y_test)\n",
    "    _,acc_decision_tree,pre_dtree,rec_dtree =  decision_tree(x_train,y_train,x_test,y_test)\n",
    "    models = pd.DataFrame({\n",
    "        'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "                  'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "                  'Stochastic Gradient Decent', 'Linear SVC', \n",
    "                  'Decision Tree'],\n",
    "        'Score': [acc_svc, acc_knn, acc_log, \n",
    "                  acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "                  acc_sgd, acc_linear_svc, acc_decision_tree],\n",
    "        'Precision': [pre_svc, pre_knn,pre_log,\n",
    "                      pre_rf,pre_gau,pre_per,pre_sgd,\n",
    "                      pre_lsvc,pre_dtree],\n",
    "        'Recall': [rec_svc,rec_knn,rec_log,rec_rf,rec_gau,rec_per,rec_sgd,rec_lsvc,rec_dtree]\n",
    "    })\n",
    "    return models #.sort_values(by='Score', ascending=False)\n",
    "\n",
    "def map_for_emotion(y_list, emotional_mapping,emotion):\n",
    "    return list(map(lambda x: 1 if x == emotional_mapping[emotion] else 0,y_list))\n",
    "\n",
    "def compare_models_x_emotions(x_train, x_test, emotions):\n",
    "    result = {'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "                  'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "                  'Stochastic Gradient Decent', 'Linear SVC', \n",
    "                  'Decision Tree']}\n",
    "    for emotion,data in emotions.iteritems():\n",
    "        y_train = data[0]\n",
    "        y_test = data[1]\n",
    "        _,acc_svc = svc(x_train,y_train,x_test,y_test)\n",
    "        _,acc_knn = knn(x_train,y_train,x_test,y_test)\n",
    "        _,acc_log = log_reg(x_train,y_train,x_test,y_test)\n",
    "        _,acc_random_forest = random_forest(x_train,y_train,x_test,y_test)\n",
    "        _,acc_gaussian = gaussian(x_train,y_train,x_test,y_test)\n",
    "        _,acc_perceptron = perceptron(x_train,y_train,x_test,y_test)\n",
    "        _,acc_sgd = sgd(x_train,y_train,x_test,y_test)\n",
    "        _,acc_linear_svc = linear_svc(x_train,y_train,x_test,y_test)\n",
    "        _,acc_decision_tree =  decision_tree(x_train,y_train,x_test,y_test)\n",
    "        result[emotion] = [acc_svc, acc_knn, acc_log, \n",
    "                  acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "                  acc_sgd, acc_linear_svc, acc_decision_tree]\n",
    "    models = pd.DataFrame(result)\n",
    "    models = models[['Model'] + list(emotions.iterkeys())]\n",
    "    return models\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full = generate_IEMOCAP_df()\n",
    "data = pd.read_csv('data/IEMOCAP_sentences.csv',index_col=0)\n",
    "data = data[(data.emotion != 'oth')&(data.emotion != 'xxx')]\n",
    "emotional_mapping = {'ang': 0, 'sad': 1, 'exc': 2, 'neu': 3,'fru': 4,'hap': 5,'fea': 6,'sur': 7,'dis': 8, 'xxx':9,'oth':10}\n",
    "data['emotion'] = data['emotion'].map( emotional_mapping ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(data,emotional_mapping,emotion):\n",
    "    emotion_code = emotional_mapping[emotion]\n",
    "    emotion_sample = data[data.emotion == emotion_code]\n",
    "    sample_size = len(emotion_sample)\n",
    "    oth_emotion_sample = data[data.emotion != emotion_code].sample(n=sample_size)\n",
    "    sample = pd.concat([oth_emotion_sample,emotion_sample]).sample(frac=1)\n",
    "    y = map_for_emotion(sample.emotion,emotional_mapping,'ang')\n",
    "    return sample,y\n",
    "\n",
    "def get_mfcc_feature(filename,n_mfcc):\n",
    "    y, sr = librosa.load(filename)\n",
    "    #IPython.display.Audio(data=y,rate=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=y,sr=sr, n_mfcc=n_mfcc)\n",
    "    print(\"Finished \"+ filename)\n",
    "    return np.mean(mfcc.T,axis=0)\n",
    "\n",
    "def play_audio(filename):\n",
    "    y, sr = librosa.load(filename)\n",
    "    return ipd.Audio(data=y,rate=sr)\n",
    "\n",
    "def get_confusion(y_pred, y_true):\n",
    "    tp = []\n",
    "    fp =[]\n",
    "    fn =[]\n",
    "    tn = []\n",
    "    for i in range(len(y_pred)):\n",
    "        if(y_pred[i] == y_true[i]):\n",
    "            if y_pred[i] == 1:\n",
    "                tp.append(True)\n",
    "                tn.append(False)\n",
    "            else:\n",
    "                tn.append(True)\n",
    "                tp.append(False)\n",
    "            fp.append(False)\n",
    "            fn.append(False)\n",
    "        else:\n",
    "            if(y_pred[i] == 1):\n",
    "                fp.append(True)\n",
    "                fn.append(False)\n",
    "            else:\n",
    "                fp.append(False)\n",
    "                fn.append(True)\n",
    "            tn.append(False)\n",
    "            tp.append(False)\n",
    "            \n",
    "    return tp,tn,fp,fn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anger Binary Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1764, 11) 1764\n",
      "(442, 11) 442\n"
     ]
    }
   ],
   "source": [
    "# x,y = get_sample(data,emotional_mapping,'ang')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "# try:\n",
    "#     with open('pickles/anger/base_datasets.pickle','wb') as f:\n",
    "#         save = {\n",
    "#             'X_train' : X_train,\n",
    "#             'X_test' : X_test,\n",
    "#             'y_train': y_train,\n",
    "#             'y_test' : y_test\n",
    "#         }\n",
    "#         pickle.dump(save,f,pickle.HIGHEST_PROTOCOL)\n",
    "# except Exception as e:\n",
    "#     print('Unable to save data to anger base datasets pickle: ', e)\n",
    "    \n",
    "try:\n",
    "    with open('pickles/anger/base_datasets.pickle','rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        X_train = save['X_train']\n",
    "        X_test = save['X_test']\n",
    "        y_train = save['y_train']\n",
    "        y_test = save['y_test']\n",
    "        del save\n",
    "except Exception as e:\n",
    "    print('Error loading base datasets pickle: ', e)\n",
    "\n",
    "print X_train.shape, len(y_train)\n",
    "print X_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# x_train = list(map(lambda x: get_mfcc_feature(x,20),X_train['wav_path']))\n",
    "# x_test = list(map(lambda x: get_mfcc_feature(x,20),X_test['wav_path']))\n",
    "# try:\n",
    "#     with open('pickles/anger/mfcc20.pickle','wb') as f:\n",
    "#         save = {\n",
    "#             'train_dataset' : x_train,\n",
    "#             'train_labels' : y_train,\n",
    "#             'test_dataset' : x_test,\n",
    "#             'test_labels' : y_test\n",
    "#         }\n",
    "#         pickle.dump(save,f,pickle.HIGHEST_PROTOCOL)\n",
    "# except Exception as e:\n",
    "#     print('Unable to save data to mfcc pickle: ', e)\n",
    "\n",
    "try:\n",
    "    with open('pickles/anger/mfcc20.pickle','rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        x_train = save['train_dataset']\n",
    "        y_train = save['train_labels']\n",
    "        x_test = save['test_dataset']\n",
    "        y_test = save['test_labels']\n",
    "        del save\n",
    "except Exception as e:\n",
    "    print('Error at:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(x_train, y_train,x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, Y_pred,acc, precision, recall = log_reg(x_train,y_train,x_test,y_test, delete = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp,tn,fp,fn = get_confusion(Y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['text','emotion']].iloc[fn].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotional_mapping = {'ang': 0, 'sad': 1, 'exc': 2, 'neu': 3,'fru': 4,'hap': 5,'fea': 6,'sur': 7,'dis': 8, 'xxx':9,'oth':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(X_test.iloc[fn]['wav_path'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, Y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Anger','NotAnger'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# # Plot normalized confusion matrix\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=['Anger','NotAnger'], normalize=True,\n",
    "#                       title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
