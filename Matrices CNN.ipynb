{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Pickling\n",
    "from six.moves import cPickle as pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "no_alignment_file = [4764]\n",
    "wrong_alignment = [3730]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roblescoulter/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, Input, Dense, Flatten, Dropout, Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from glove import Corpus, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patterns(data,extract=False):\n",
    "    if(extract):\n",
    "        patterns = {}\n",
    "        for index, row in data.iterrows():\n",
    "            patterns[row['index']] = set(get_pattern([row['text']])[0].values())\n",
    "            print('Extracted pattern from '+ row['index'] + ' index:'+ str(index))\n",
    "            print('Size: ', len(patterns[row['index']]), 'Patterns size', len(patterns))\n",
    "        try:\n",
    "            print('Saving Pickle')\n",
    "            with open('pickles/patterns/pattern.pickle','wb') as f:\n",
    "                save = {\n",
    "                    'patterns' : patterns\n",
    "                }\n",
    "                pickle.dump(save,f,pickle.HIGHEST_PROTOCOL)\n",
    "                print('Successfully saved in pattern.pickle')\n",
    "                return patterns\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to pickle', e)\n",
    "            print('Patterns probably not saved.')\n",
    "            return patterns\n",
    "    else:\n",
    "        try:\n",
    "            with open('pickles/patterns/pattern.pickle','rb') as f:\n",
    "                save = pickle.load(f)\n",
    "                patterns = save['patterns']\n",
    "                del save\n",
    "                returning = {}\n",
    "                for key in list(data['index']):\n",
    "                    returning[key] = patterns[key]\n",
    "                return returning\n",
    "        except Exception as e:\n",
    "            print('Error loading base datasets pickle: ', e)\n",
    "            \n",
    "def clean_text(text, remove_actions= True):\n",
    "    punct_str = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~«»“…‘”'\n",
    "    if(remove_actions):\n",
    "        text = re.sub(r\" ?\\[[^)]+\\]\", \"\", text)\n",
    "    for p in punct_str:\n",
    "        text = text.replace(p,' ')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def filter_word_count(data, n_count):\n",
    "    return data[list(map(lambda x: len(x.split(' ')) >= n_count,data['text']))]\n",
    "\n",
    "\n",
    "def remove_empty_patterns(data,patterns):\n",
    "    empty_patterns = [k for k, v in patterns.items() if len(v) < 1]\n",
    "    patterns = { k:v for k, v in patterns.items() if len(v) >= 1 }\n",
    "    data = filter(lambda x: x[1]['index'] not in empty_patterns ,data.iterrows())\n",
    "    data = pd.DataFrame.from_items(data).T\n",
    "    return data,patterns\n",
    "\n",
    "def remove_multiwildcard(patterns):\n",
    "    for index, patt in patterns.items():\n",
    "        flt_patt = {p for p in patt if p.split(' ').count('.+') == 1}\n",
    "        patterns[index] = flt_patt\n",
    "    return patterns\n",
    "\n",
    "def load_data(word_count,emotional_mapping):\n",
    "    # full = generate_IEMOCAP_df()\n",
    "    data = pd.read_csv('data/IEMOCAP_sentences_votebased.csv',index_col=0)\n",
    "    data['emotion_code'] = data['emotion'].map( emotional_mapping ).astype(int)\n",
    "    # Take away fear, surprise,disgust, xxx and others. Not enough data\n",
    "    data = data[data.emotion_code < 4]\n",
    "    #Remove rows that don't have Alignment file\n",
    "    try:\n",
    "        data = data.drop(no_alignment_file)\n",
    "    except Exception as e:\n",
    "        print('Error at: ',e)\n",
    "    # Remove rows that have wrong Alignment file\n",
    "    try:\n",
    "        data = data.drop(wrong_alignment)\n",
    "    except Exception as e:\n",
    "        print('Error at: ',e)\n",
    "#     Clean Transcripts\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "    # Filter Word Count\n",
    "    data = filter_word_count(data, word_count)\n",
    "    patterns = extract_patterns(data)\n",
    "    data,patterns = remove_empty_patterns(data,patterns)\n",
    "    patterns = remove_multiwildcard(patterns)\n",
    "    return data,patterns\n",
    "\n",
    "def load_acoustic_fullmatrices(extraction_type = 'full',extract_fd = False):\n",
    "    if(extraction_type in ['full','wc','cw']):\n",
    "        try:\n",
    "            if(extract_fd):\n",
    "                fullmfcc_matrix_fd = None\n",
    "                fullrmse_matrix_fd = pd.read_pickle('pickles/patterns/'+extraction_type+'_rmse_matrix_fd.pickle')\n",
    "                print('Successfully loaded '+extraction_type+' RMSE Matrix FULLDATA')\n",
    "                fullzcr_matrix_fd = pd.read_pickle('pickles/patterns/'+extraction_type+'_zcr_matrix_fd.pickle')\n",
    "                print('Successfully loaded '+extraction_type+' ZCR Matrix FULLDATA')   \n",
    "                with open('pickles/patterns/'+extraction_type+'_mfcc20_matrix_fd.pickle','rb') as f:\n",
    "                    save = pickle.load(f)\n",
    "                    fullmfcc_matrix_fd = save['multimatrix']\n",
    "                    del save\n",
    "                print('Successfully loaded '+extraction_type+' MFCC Matrices FULLDATA')\n",
    "                fullmfcc_matrix_fd.append(fullrmse_matrix_fd)\n",
    "                fullmfcc_matrix_fd.append(fullzcr_matrix_fd)\n",
    "                return fullmfcc_matrix_fd\n",
    "            else:\n",
    "                fullmfcc_matrix = None\n",
    "                fullrmse_matrix = pd.read_pickle('pickles/patterns/'+extraction_type+'_rmse_matrix.pickle')\n",
    "                print('Successfully loaded '+extraction_type+' RMSE Matrix')   \n",
    "                fullzcr_matrix = pd.read_pickle('pickles/patterns/'+extraction_type+'_zcr_matrix.pickle')\n",
    "                print('Successfully loaded '+extraction_type+' ZCR Matrix')\n",
    "                with open('pickles/patterns/'+extraction_type+'_mfcc20_matrix.pickle','rb') as f:\n",
    "                    save = pickle.load(f)\n",
    "                    fullmfcc_matrix = save['multimatrix']\n",
    "                    del save\n",
    "                print('Successfully loaded '+extraction_type+' MFCC Matrices') \n",
    "                fullmfcc_matrix.append(fullrmse_matrix)\n",
    "                fullmfcc_matrix.append(fullzcr_matrix)\n",
    "                return fullmfcc_matrix\n",
    "        except Exception as e:\n",
    "            print('Error loading matrix: ', e)\n",
    "    else:\n",
    "        print('Error')\n",
    "        return None,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at:  labels [4764] not contained in axis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "ang    1141\n",
       "hap     680\n",
       "neu    1440\n",
       "sad     947\n",
       "Name: index, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATASET\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "emotional_mapping = {'ang': 0, 'sad': 1, 'hap': 2, 'neu': 3,'fru': 4,'exc': 5,'fea': 6,'sur': 7,'dis': 8, 'xxx':9,'oth':10}\n",
    "\n",
    "data, patterns = load_data(3,emotional_mapping)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(data, data.emotion_code, test_size=TEST_SIZE)\n",
    "try:\n",
    "    with open('pickles/matrix_basedata.pickle','rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        X_train = save['X_train']\n",
    "        X_test = save['X_test']\n",
    "        y_train = save['y_train']\n",
    "        y_test = save['y_test']\n",
    "        del save\n",
    "except Exception as e:\n",
    "    print('Error loading base datasets pickle: ', e)\n",
    "\n",
    "data.groupby('emotion').count()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded full RMSE Matrix\n",
      "Successfully loaded full ZCR Matrix\n",
      "Successfully loaded full MFCC Matrices\n",
      "Successfully loaded wc RMSE Matrix\n",
      "Successfully loaded wc ZCR Matrix\n",
      "Successfully loaded wc MFCC Matrices\n",
      "Successfully loaded cw RMSE Matrix\n",
      "Successfully loaded cw ZCR Matrix\n",
      "Successfully loaded cw MFCC Matrices\n",
      "5853 22 22 22\n"
     ]
    }
   ],
   "source": [
    "full_matrices = load_acoustic_fullmatrices(extraction_type='full')\n",
    "wc_matrices = load_acoustic_fullmatrices(extraction_type='wc')\n",
    "cw_matrices = load_acoustic_fullmatrices(extraction_type='cw')\n",
    "##################### FULLDATA ACOUSTIC MATRICES ###################################\n",
    "# fullmfcc_matrices_fd = load_acoustic_fullmatrices(extraction_type='full',extract_fd = True)\n",
    "# wcmfcc_matrices_fd = load_acoustic_fullmatrices(extraction_type='wc',extract_fd = True)\n",
    "# cwmfcc_matrices_fd = load_acoustic_fullmatrices(extraction_type='cw',extract_fd = True)\n",
    "########################################################################################\n",
    "RMSE_INDEX = 20\n",
    "ZCR_INDEX = 21\n",
    "###########################################################################################\n",
    "\n",
    "em_df = pd.read_pickle('pickles/patterns/pfief_matrix.pickle')\n",
    "\n",
    "patterns_list = np.array(list(em_df.index))\n",
    "print(len(em_df),len(full_matrices),len(wc_matrices),len(cw_matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_vectors(data,patterns_list):\n",
    "    patterns = extract_patterns(data)\n",
    "    transcript_order = list(data['index'])\n",
    "    frequency_vectors = []\n",
    "    for index in patterns:\n",
    "        frequency_vectors.append(np.isin(patterns_list,np.array(list(patterns[index]))))\n",
    "    vectors = pd.DataFrame(frequency_vectors,columns=patterns_list,index=patterns.keys())\n",
    "    vectors = vectors.loc[transcript_order]\n",
    "    vectors = vectors * 1\n",
    "    return vectors\n",
    "\n",
    "vectors = get_frequency_vectors(X_train,patterns_list)\n",
    "test_vectors = get_frequency_vectors(X_test,patterns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.+ a</th>\n",
       "      <th>.+ a big</th>\n",
       "      <th>.+ a bit</th>\n",
       "      <th>.+ a day</th>\n",
       "      <th>.+ a dog</th>\n",
       "      <th>.+ a few</th>\n",
       "      <th>.+ a good</th>\n",
       "      <th>.+ a guy</th>\n",
       "      <th>.+ a little</th>\n",
       "      <th>.+ a lot</th>\n",
       "      <th>...</th>\n",
       "      <th>your .+ with</th>\n",
       "      <th>your .+ you</th>\n",
       "      <th>your .+ your</th>\n",
       "      <th>your a .+</th>\n",
       "      <th>your life .+</th>\n",
       "      <th>your name .+</th>\n",
       "      <th>your own .+</th>\n",
       "      <th>yours .+</th>\n",
       "      <th>yourself .+</th>\n",
       "      <th>yourself a .+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ses04F_script03_2_M001</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ses05M_script02_1_F016</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ses04F_script01_2_F013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ses02F_script03_1_F010</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ses03M_impro08a_F014</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5853 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       .+ a .+ a big .+ a bit .+ a day .+ a dog .+ a few  \\\n",
       "Ses04F_script03_2_M001    0        0        0        0        0        0   \n",
       "Ses05M_script02_1_F016    0        0        0        0        0        0   \n",
       "Ses04F_script01_2_F013    0        0        0        0        0        0   \n",
       "Ses02F_script03_1_F010    0        0        0        0        0        0   \n",
       "Ses03M_impro08a_F014      1        0        0        0        0        0   \n",
       "\n",
       "                       .+ a good .+ a guy .+ a little .+ a lot      ...       \\\n",
       "Ses04F_script03_2_M001         0        0           0        0      ...        \n",
       "Ses05M_script02_1_F016         0        0           0        0      ...        \n",
       "Ses04F_script01_2_F013         0        0           0        0      ...        \n",
       "Ses02F_script03_1_F010         0        0           0        0      ...        \n",
       "Ses03M_impro08a_F014           0        0           0        0      ...        \n",
       "\n",
       "                       your .+ with your .+ you your .+ your your a .+  \\\n",
       "Ses04F_script03_2_M001            0           0            0         0   \n",
       "Ses05M_script02_1_F016            0           0            0         0   \n",
       "Ses04F_script01_2_F013            0           0            0         0   \n",
       "Ses02F_script03_1_F010            0           0            0         0   \n",
       "Ses03M_impro08a_F014              0           0            0         0   \n",
       "\n",
       "                       your life .+ your name .+ your own .+ yours .+  \\\n",
       "Ses04F_script03_2_M001            0            0           0        0   \n",
       "Ses05M_script02_1_F016            0            0           0        0   \n",
       "Ses04F_script01_2_F013            0            0           0        0   \n",
       "Ses02F_script03_1_F010            0            0           0        0   \n",
       "Ses03M_impro08a_F014              0            0           0        0   \n",
       "\n",
       "                       yourself .+ yourself a .+  \n",
       "Ses04F_script03_2_M001           0             0  \n",
       "Ses05M_script02_1_F016           0             0  \n",
       "Ses04F_script01_2_F013           0             0  \n",
       "Ses02F_script03_1_F010           0             0  \n",
       "Ses03M_impro08a_F014             0             0  \n",
       "\n",
       "[5 rows x 5853 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.+ a</th>\n",
       "      <th>.+ a big</th>\n",
       "      <th>.+ a bit</th>\n",
       "      <th>.+ a day</th>\n",
       "      <th>.+ a dog</th>\n",
       "      <th>.+ a few</th>\n",
       "      <th>.+ a good</th>\n",
       "      <th>.+ a guy</th>\n",
       "      <th>.+ a little</th>\n",
       "      <th>.+ a lot</th>\n",
       "      <th>...</th>\n",
       "      <th>your .+ with</th>\n",
       "      <th>your .+ you</th>\n",
       "      <th>your .+ your</th>\n",
       "      <th>your a .+</th>\n",
       "      <th>your life .+</th>\n",
       "      <th>your name .+</th>\n",
       "      <th>your own .+</th>\n",
       "      <th>yours .+</th>\n",
       "      <th>yourself .+</th>\n",
       "      <th>yourself a .+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ses05F_impro02_F024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ses01M_script01_1_F014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ses03F_impro04_M019</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ses02M_impro05_M013</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ses03F_impro08_M002</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5853 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       .+ a .+ a big .+ a bit .+ a day .+ a dog .+ a few  \\\n",
       "Ses05F_impro02_F024       0        0        0        0        0        0   \n",
       "Ses01M_script01_1_F014    0        0        0        0        0        0   \n",
       "Ses03F_impro04_M019       0        0        0        0        0        0   \n",
       "Ses02M_impro05_M013       0        0        0        0        0        0   \n",
       "Ses03F_impro08_M002       0        0        0        0        0        0   \n",
       "\n",
       "                       .+ a good .+ a guy .+ a little .+ a lot      ...       \\\n",
       "Ses05F_impro02_F024            0        0           0        0      ...        \n",
       "Ses01M_script01_1_F014         0        0           0        0      ...        \n",
       "Ses03F_impro04_M019            0        0           0        0      ...        \n",
       "Ses02M_impro05_M013            0        0           0        0      ...        \n",
       "Ses03F_impro08_M002            0        0           0        0      ...        \n",
       "\n",
       "                       your .+ with your .+ you your .+ your your a .+  \\\n",
       "Ses05F_impro02_F024               0           0            0         0   \n",
       "Ses01M_script01_1_F014            0           0            0         0   \n",
       "Ses03F_impro04_M019               0           0            0         0   \n",
       "Ses02M_impro05_M013               0           0            0         0   \n",
       "Ses03F_impro08_M002               0           0            0         0   \n",
       "\n",
       "                       your life .+ your name .+ your own .+ yours .+  \\\n",
       "Ses05F_impro02_F024               0            0           0        0   \n",
       "Ses01M_script01_1_F014            0            0           0        0   \n",
       "Ses03F_impro04_M019               0            0           0        0   \n",
       "Ses02M_impro05_M013               0            0           0        0   \n",
       "Ses03F_impro08_M002               0            0           0        0   \n",
       "\n",
       "                       yourself .+ yourself a .+  \n",
       "Ses05F_impro02_F024              0             0  \n",
       "Ses01M_script01_1_F014           0             0  \n",
       "Ses03F_impro04_M019              0             0  \n",
       "Ses02M_impro05_M013              0             0  \n",
       "Ses03F_impro08_M002              0             0  \n",
       "\n",
       "[5 rows x 5853 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING\n",
    "EMBEDDING_DIM  = 4\n",
    "MAX_SEQ_LENGTH = 170\n",
    "\n",
    "# MODEL\n",
    "FILTER_SIZES   = [1,2,3]\n",
    "FEATURE_MAPS   = [100,100,100]\n",
    "DROPOUT_RATE   = 0.5\n",
    "\n",
    "# LEARNING\n",
    "BATCH_SIZE     = 200\n",
    "NB_EPOCHS      = 50\n",
    "RUNS           = 5\n",
    "VAL_SIZE       = 0.2\n",
    "LEARNING_RATE  = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text informations:\n",
      "max length: 165 / min length: 1 / mean length: 25 / limit length: 170\n"
     ]
    }
   ],
   "source": [
    "result = [np.sum(x[1]) for x in vectors.iterrows()]\n",
    "\n",
    "print('Text informations:')\n",
    "print('max length: %i / min length: %i / mean length: %i / limit length: %i' % (np.max(result),\n",
    "                                                                                np.min(result),\n",
    "                                                                                np.mean(result),\n",
    "                                                                                MAX_SEQ_LENGTH))\n",
    "data = []\n",
    "for key, row in vectors.iterrows():\n",
    "    row_patt = [ i for i,v in row.iteritems() if v == 1]\n",
    "    row_matrix = em_df.loc[row_patt,:].as_matrix()\n",
    "    pad = np.zeros((MAX_SEQ_LENGTH,EMBEDDING_DIM))\n",
    "    pad[:row_matrix.shape[0],:row_matrix.shape[1]] = row_matrix\n",
    "    data.append(pad)\n",
    "    \n",
    "test_data = []\n",
    "for key, row in test_vectors.iterrows():\n",
    "    row_patt = [ i for i,v in row.iteritems() if v == 1]\n",
    "    row_matrix = em_df.loc[row_patt,:].as_matrix()\n",
    "    pad = np.zeros((MAX_SEQ_LENGTH,EMBEDDING_DIM))\n",
    "    pad[:row_matrix.shape[0],:row_matrix.shape[1]] = row_matrix\n",
    "    test_data.append(pad)\n",
    "# data = []\n",
    "# for key, row in x_train.iterrows():\n",
    "#     features = scaled_feature_table[row['index']]\n",
    "#     sorted(features)\n",
    "# #     sequences.append(features.values())\n",
    "#     feat_matrix = np.array(features.values())\n",
    "#     only_mfcc = feat_matrix[:,:20]\n",
    "#     pad = np.zeros((MAX_SEQ_LENGTH,EMBEDDING_DIM))\n",
    "#     pad[:feat_matrix.shape[0],:feat_matrix.shape[1]] = only_mfcc#feat_matrix\n",
    "#     data.append(pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ee5640db5ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# row_patt = [ i for i,v in row.iteritems() if v == 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#row_matrix = em_df.loc[row_patt,:].as_matrix()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         result = _get_dummies_1d(data, prefix, prefix_sep, dummy_na,\n\u001b[0;32m-> 1215\u001b[0;31m                                  sparse=sparse, drop_first=drop_first)\n\u001b[0m\u001b[1;32m   1216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first)\u001b[0m\n\u001b[1;32m   1220\u001b[0m                     sparse=False, drop_first=False):\n\u001b[1;32m   1221\u001b[0m     \u001b[0;31m# Series avoids inconsistent NaN handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m     \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factorize_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_empty_Frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 data = _sanitize_array(data, index, dtype, copy,\n\u001b[0;32m--> 264\u001b[0;31m                                        raise_cast_failure=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_sanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m   3273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3275\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data must be 1-dimensional'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3276\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3277\u001b[0m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_tuplesafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# row_patt = [ i for i,v in row.iteritems() if v == 1]\n",
    "#row_matrix = em_df.loc[row_patt,:].as_matrix()\n",
    "y_train = pd.get_dummies(y_train).values\n",
    "y_test = pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(data).shape, y_train.shape, np.array(test_data).shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ap_cnn_model\n",
    "import time  \n",
    "\n",
    "FILTER_SIZES   = [1,2]\n",
    "FEATURE_MAPS   = [50,50]\n",
    "DROPOUT_RATE   = 0.5\n",
    "LEARNING_RATE  = 0.01\n",
    "histories = []\n",
    "for i in range(RUNS):\n",
    "    print('Running iteration %i/%i' % (i+1, RUNS))\n",
    "    start_time = time.time()\n",
    "    emb_layer = None\n",
    "    \n",
    "    model = ap_cnn_model.build_cnn(\n",
    "        embedding_dim= EMBEDDING_DIM,\n",
    "        filter_sizes = FILTER_SIZES,\n",
    "        feature_maps = FEATURE_MAPS,\n",
    "        max_seq_length = MAX_SEQ_LENGTH,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adadelta(clipvalue=3,lr=LEARNING_RATE),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        [data], y_train,\n",
    "        epochs=NB_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        validation_data=([test_data], y_test),\n",
    "        callbacks=[ModelCheckpoint('model-%i.h5'%(i+1), monitor='val_loss',\n",
    "                                   verbose=0, save_best_only=True, mode='min'),\n",
    "                   ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, min_lr=0.01)\n",
    "                  ]\n",
    "    )\n",
    "    histories.append(history.history)\n",
    "    print('Iteration', i+1)\n",
    "    print(\"--- %s seconds on ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg(histories, his_key):\n",
    "    tmp = []\n",
    "    for history in histories:\n",
    "        tmp.append(history[his_key][np.argmin(history['val_loss'])])\n",
    "    return np.mean(tmp)\n",
    "\n",
    "def get_scores_from_multiple(history_folder):\n",
    "    V_folder = os.listdir(history_folder)\n",
    "    values = []\n",
    "    for pkl in V_folder:\n",
    "        history = pickle.load(open(history_folder+pkl, 'rb'))\n",
    "        values.append([pkl,get_avg(history,'acc'),get_avg(history,'val_acc'),get_avg(history,'loss'),get_avg(history,'val_loss')])\n",
    "    # histories = pickle.load(open('history/unbalanced_glovetext8_2500.pkl', 'rb'))\n",
    "    scores = pd.DataFrame(data=values,columns=['Model','Train Acc','Valid Acc','Train Loss','Valid Loss'])\n",
    "    return scores\n",
    "\n",
    "def  plot_acc_loss(title, histories, key_acc, key_loss):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    # Accuracy\n",
    "    ax1.set_title('Model accuracy (%s)' % title)\n",
    "    names = []\n",
    "    for i, model in enumerate(histories):\n",
    "        ax1.plot(model[key_acc])\n",
    "        ax1.set_xlabel('epoch')\n",
    "        names.append('Model %i' % (i+1))\n",
    "        ax1.set_ylabel('accuracy')\n",
    "    ax1.legend(names, loc='lower right')\n",
    "    # Loss\n",
    "    ax2.set_title('Model loss (%s)' % title)\n",
    "    for model in histories:\n",
    "        ax2.plot(model[key_loss])\n",
    "        ax2.set_xlabel('epoch')\n",
    "        ax2.set_ylabel('loss')\n",
    "    ax2.legend(names, loc='upper right')\n",
    "    fig.set_size_inches(20, 5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training: \\t%0.4f loss / %0.4f acc' % (get_avg(histories, 'loss'),\n",
    "                                              get_avg(histories, 'acc')))\n",
    "print('Validation: \\t%0.4f loss / %0.4f acc' % (get_avg(histories, 'val_loss'),\n",
    "                                                get_avg(histories, 'val_acc')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss('training', histories, 'acc', 'loss')\n",
    "plot_acc_loss('validation', histories, 'val_acc', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
