{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "# from glove import Corpus, Glove\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "# from sklearn.svm import SVC\n",
    "# import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "# from collections import Counter, defaultdict\n",
    "# from tabulate import tabulate\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_glove(data, LEARNING_RATE=0.05, EPOCHS=5, NO_THREADS=4, EMBEDDING_DIM=100):\n",
    "    model = None\n",
    "    corpus = Corpus()\n",
    "    corpus.fit(data, window=10)\n",
    "    model = Glove(no_components=EMBEDDING_DIM,learning_rate=LEARNING_RATE)\n",
    "    model.fit(corpus.matrix, epochs=EPOCHS,no_threads=NO_THREADS,verbose=True)\n",
    "    model.add_dictionary(corpus.dictionary)\n",
    "    return model\n",
    "\n",
    "def create_word2vec(data,EMBEDDING_DIM=100):\n",
    "    model = word2vec.Word2Vec(data, size=EMBEDDING_DIM)\n",
    "    return model\n",
    "\n",
    "def preprocess_text(posts):\n",
    "    text = str(posts['post_title'])+'. '+ str(posts['post_text'])\n",
    "    text =  re.sub('tl[;]?dr','',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+[0-9]+[s]?[ /\\(,)]*f[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+[0-9]+[s]?[ /\\(,)]*m[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+f[ /\\(,)]*[0-9]+[s]?[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+m[ /\\(,)]*[0-9]+[s]?[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[0-9]+','NUM',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('u/[^\\s]+','AT_USER',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',text,flags=re.IGNORECASE)  #Convert www.* or https?://* to <url>\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*[eE]dit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*EDIT\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*big edit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*important edit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*[uU]pdate\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*UPDATE\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*big update\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*important update\")[0]\n",
    "    text = text.split(\"[.]?\\nfor an update\")[0]\n",
    "    text = text.replace('\\r', '')\n",
    "    return text\n",
    "\n",
    "#calculate two things,\n",
    "#specificness how good is the cluster and the elements similar to each other/ how well can other elements be described by their neighbors\n",
    "#uniqueness is how differnet this cluster to others\n",
    "def cluster_score(clean_cluster,c_syn,k):\n",
    "    unique_clusters = clean_cluster.loc[:,['cluster']]\n",
    "    unique_clusters = unique_clusters.drop_duplicates()\n",
    "    tot_spec = 0\n",
    "    tot_uni = 0\n",
    "    for num,c in unique_clusters.iterrows():\n",
    "        union = clean_cluster[clean_cluster['cluster']==c['cluster']].merge(c_syn[c_syn['cluster']==c['cluster']],how='inner',left_on=['word'],right_on=['syn'])\n",
    "        specificness = len(union)/len(clean_cluster[clean_cluster['cluster']==c['cluster']])\n",
    "        union = clean_cluster[clean_cluster['cluster']!=c['cluster']].merge(c_syn[c_syn['cluster']==c['cluster']],how='inner',left_on=['word'],right_on=['syn'])\n",
    "        uniqueness = 1 - (len(union)/(len(clean_cluster)-len(clean_cluster[clean_cluster['cluster']==c['cluster']])))\n",
    "        tot_spec = tot_spec + specificness\n",
    "        tot_uni = tot_uni + uniqueness\n",
    "    tot_spec = tot_spec/len(unique_clusters)\n",
    "    tot_uni = tot_uni/len(unique_clusters)\n",
    "    return {'spec':tot_spec,'uni':tot_uni,'k':k}\n",
    "\n",
    "    \n",
    "def generate_syn_info(cluster):\n",
    "    cluster_syn = pd.DataFrame()\n",
    "    unique_clusters = cluster.loc[:,['cluster']]\n",
    "    unique_clusters = unique_clusters.drop_duplicates()\n",
    "    for cnum,c in unique_clusters.iterrows():\n",
    "#         print('starting cluster...',c[0])\n",
    "        cur_cluster = cluster[cluster['cluster']==c['cluster']]\n",
    "        syns = []\n",
    "        for wnum,word in cur_cluster.iterrows():\n",
    "            for s in wn.synsets(word['word']):\n",
    "                syn = s.name().split('.')[0]\n",
    "                if syn.find('_')<0:  #filter out composed words\n",
    "                    syns.append(syn)\n",
    "            #syns.append(word['word'])\n",
    "        this_cluster = pd.DataFrame(syns,columns=['syn'])\n",
    "        this_cluster['cluster'] = c[0]\n",
    "        this_cluster = this_cluster.drop_duplicates()\n",
    "        cluster_syn = pd.concat([cluster_syn,this_cluster])         \n",
    "    return cluster_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359557"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare sentences\n",
    "c_train = pd.read_csv('data/c_train2.csv')\n",
    "c_test = pd.read_csv('data/c_test2.csv')\n",
    "c_data = pd.concat([c_train,c_test],sort=False)\n",
    "c_data = c_data.loc[:,['post_created_utc', 'full_link', 'post_id', 'post_num_comments',\n",
    "       'post_score', 'subreddit', 'post_title', 'post_text']]\n",
    "\n",
    "nc_train = pd.read_csv('data/nc_train2.csv')\n",
    "nc_test = pd.read_csv('data/nc_test2.csv')\n",
    "nc_data = pd.concat([nc_train,nc_test],sort=False)\n",
    "nc_data = nc_data.loc[:,['post_created_utc', 'full_link', 'post_id', 'post_num_comments',\n",
    "       'post_score', 'subreddit', 'post_title', 'post_text']]\n",
    "\n",
    "full_data = pd.concat([c_data,nc_data],sort=False)\n",
    "full_data = full_data.sample(len(full_data))\n",
    "posts = full_data.apply(preprocess_text,axis=1)\n",
    "data_sentences = []\n",
    "for post in posts:\n",
    "    sent_tokenize_list = sent_tokenize(post)\n",
    "    data = [nltk.word_tokenize(sentence) for sentence in sent_tokenize_list]\n",
    "    data_sentences = data_sentences + data \n",
    "len(data_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "print('start training...')\n",
    "# gloveModel = create_glove(data_sentences)\n",
    "w2vModel = create_word2vec(data_sentences)\n",
    "#is ok to train the model with the full dataset as we are not providing labels.\n",
    "w2v = {w: vec for w, vec in zip(w2vModel.wv.index2word, w2vModel.wv.syn0)}\n",
    "# glove = {w: vec for w, vec in zip(gloveModel.dictionary, gloveModel.word_vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.22433679958269076\n"
     ]
    }
   ],
   "source": [
    "#clustering w2v\n",
    "print (w2vModel.similarity('this', 'is'))\n",
    "# w2vModel.wv.index2word\n",
    "# print (w2vModel.most_similar(positive=['hello'], negative=[], topn=10))\n",
    "\n",
    "# w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from nltk.cluster import KMeansClusterer\n",
    "# import nltk\n",
    "# X = w2vModel[w2vModel.wv.index2word]\n",
    "# NUM_CLUSTERS=10\n",
    "# kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "# assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "# # print (assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words = pd.DataFrame(list(w2vModel.wv.index2word),columns=['word'])\n",
    "# clusters = pd.DataFrame(list(assigned_clusters),columns=['cluster'])\n",
    "# features = pd.DataFrame(w2vModel.wv.syn0)\n",
    "\n",
    "# result = words.merge(clusters,left_index=True,right_index=True)\n",
    "# result = result.merge(features,left_index=True,right_index=True)\n",
    "# result.to_csv('nltk_clusters10.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import cluster\n",
    "# from sklearn import metrics\n",
    "\n",
    "\n",
    "# X = w2vModel[w2vModel.wv.index2word]\n",
    "# NUM_CLUSTERS = 150\n",
    "# kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "# kmeans.fit(X)\n",
    "# labels = kmeans.labels_\n",
    "# words = pd.DataFrame(list(w2vModel.wv.index2word),columns=['word'])\n",
    "# clusters = pd.DataFrame(list(labels),columns=['cluster'])\n",
    "# features = pd.DataFrame(w2vModel.wv.vectors)\n",
    "# result = words.merge(clusters,left_index=True,right_index=True)\n",
    "# result = result.merge(features,left_index=True,right_index=True)\n",
    "    \n",
    "# clean_cluster = result\n",
    "# clean_cluster = clean_cluster[clean_cluster['word']==clean_cluster['word']]\n",
    "\n",
    "# for num,c in result.iterrows():\n",
    "#     try:\n",
    "#         w1 = wn.synsets(c['word'])\n",
    "#         if len(w1)==0:\n",
    "#             clean_cluster = clean_cluster[clean_cluster['word']!=c['word']]\n",
    "#     except:\n",
    "#         clean_cluster = clean_cluster[clean_cluster['word']!=c['word']]\n",
    "#     clean_cluster['word'] = clean_cluster.apply(lambda row: row['word'].lower(),axis=1)\n",
    "#     clean_cluster = clean_cluster.loc[:,['word','cluster']]\n",
    "#     clean_cluster = clean_cluster.drop_duplicates()\n",
    "    \n",
    "#     c_syn = generate_syn_info(clean_cluster)\n",
    "#     score = cluster_score(clean_cluster,c_syn,NUM_CLUSTERS)\n",
    "#     final_vals = final_vals.append(score,ignore_index=True)\n",
    "# #     final_vals.to_csv('cluster_eval.csv',encoding='utf-8',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('regretful.a.01'),\n",
       " Synset('deplorable.s.01'),\n",
       " Synset('good-for-nothing.s.01'),\n",
       " Synset('blue.s.08')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_vals\n",
    "wn.synsets('sorry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cluster</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>got</td>\n",
       "      <td>79</td>\n",
       "      <td>0.053381</td>\n",
       "      <td>-1.936365</td>\n",
       "      <td>0.561727</td>\n",
       "      <td>-1.378119</td>\n",
       "      <td>0.132917</td>\n",
       "      <td>5.273778</td>\n",
       "      <td>-0.025414</td>\n",
       "      <td>0.231150</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.206438</td>\n",
       "      <td>-2.622548</td>\n",
       "      <td>0.020733</td>\n",
       "      <td>-0.913719</td>\n",
       "      <td>0.762811</td>\n",
       "      <td>-2.355414</td>\n",
       "      <td>-0.921598</td>\n",
       "      <td>-2.747892</td>\n",
       "      <td>0.625829</td>\n",
       "      <td>1.551328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>went</td>\n",
       "      <td>79</td>\n",
       "      <td>0.042044</td>\n",
       "      <td>-1.707980</td>\n",
       "      <td>1.142836</td>\n",
       "      <td>-0.004915</td>\n",
       "      <td>0.386709</td>\n",
       "      <td>3.767817</td>\n",
       "      <td>-1.365376</td>\n",
       "      <td>3.451374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.635770</td>\n",
       "      <td>-0.976915</td>\n",
       "      <td>3.296836</td>\n",
       "      <td>-0.683126</td>\n",
       "      <td>1.684834</td>\n",
       "      <td>1.090186</td>\n",
       "      <td>-0.055105</td>\n",
       "      <td>-0.434413</td>\n",
       "      <td>0.496267</td>\n",
       "      <td>0.416987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>left</td>\n",
       "      <td>79</td>\n",
       "      <td>-1.456957</td>\n",
       "      <td>-1.699758</td>\n",
       "      <td>1.478573</td>\n",
       "      <td>-1.753642</td>\n",
       "      <td>-0.371985</td>\n",
       "      <td>2.490468</td>\n",
       "      <td>-0.260113</td>\n",
       "      <td>1.041301</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.268819</td>\n",
       "      <td>1.306178</td>\n",
       "      <td>0.626147</td>\n",
       "      <td>-1.218425</td>\n",
       "      <td>0.605352</td>\n",
       "      <td>0.320120</td>\n",
       "      <td>0.584418</td>\n",
       "      <td>-0.003564</td>\n",
       "      <td>-0.069379</td>\n",
       "      <td>-0.068601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>found</td>\n",
       "      <td>79</td>\n",
       "      <td>2.265918</td>\n",
       "      <td>-0.968295</td>\n",
       "      <td>-1.647641</td>\n",
       "      <td>0.484722</td>\n",
       "      <td>-1.522750</td>\n",
       "      <td>4.678810</td>\n",
       "      <td>0.581851</td>\n",
       "      <td>-1.220346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228626</td>\n",
       "      <td>-0.747722</td>\n",
       "      <td>1.109239</td>\n",
       "      <td>-1.819456</td>\n",
       "      <td>0.687684</td>\n",
       "      <td>0.321745</td>\n",
       "      <td>-2.132119</td>\n",
       "      <td>-1.349787</td>\n",
       "      <td>-1.379100</td>\n",
       "      <td>0.301172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>came</td>\n",
       "      <td>79</td>\n",
       "      <td>-0.463607</td>\n",
       "      <td>-2.136055</td>\n",
       "      <td>1.442872</td>\n",
       "      <td>0.372457</td>\n",
       "      <td>0.354493</td>\n",
       "      <td>4.310445</td>\n",
       "      <td>0.089965</td>\n",
       "      <td>1.507177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.663894</td>\n",
       "      <td>-0.929912</td>\n",
       "      <td>1.818840</td>\n",
       "      <td>-0.628678</td>\n",
       "      <td>0.912052</td>\n",
       "      <td>-0.169193</td>\n",
       "      <td>-0.065466</td>\n",
       "      <td>-2.231793</td>\n",
       "      <td>-0.408411</td>\n",
       "      <td>1.179361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>took</td>\n",
       "      <td>79</td>\n",
       "      <td>1.114481</td>\n",
       "      <td>-0.710591</td>\n",
       "      <td>-0.304834</td>\n",
       "      <td>-1.296521</td>\n",
       "      <td>0.667023</td>\n",
       "      <td>2.175258</td>\n",
       "      <td>-1.702568</td>\n",
       "      <td>1.774679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351849</td>\n",
       "      <td>-0.873941</td>\n",
       "      <td>2.864876</td>\n",
       "      <td>0.223393</td>\n",
       "      <td>1.311705</td>\n",
       "      <td>0.827495</td>\n",
       "      <td>0.233130</td>\n",
       "      <td>-2.768774</td>\n",
       "      <td>0.957672</td>\n",
       "      <td>-1.084769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>comes</td>\n",
       "      <td>79</td>\n",
       "      <td>-1.403478</td>\n",
       "      <td>-1.069754</td>\n",
       "      <td>0.811411</td>\n",
       "      <td>1.503335</td>\n",
       "      <td>0.009120</td>\n",
       "      <td>2.599219</td>\n",
       "      <td>-0.858298</td>\n",
       "      <td>-0.852993</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244975</td>\n",
       "      <td>0.126007</td>\n",
       "      <td>-0.506222</td>\n",
       "      <td>-0.422775</td>\n",
       "      <td>0.281179</td>\n",
       "      <td>0.217244</td>\n",
       "      <td>1.608713</td>\n",
       "      <td>0.263210</td>\n",
       "      <td>0.448479</td>\n",
       "      <td>2.090702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>goes</td>\n",
       "      <td>79</td>\n",
       "      <td>0.390273</td>\n",
       "      <td>-0.666014</td>\n",
       "      <td>0.277956</td>\n",
       "      <td>1.113418</td>\n",
       "      <td>0.499086</td>\n",
       "      <td>2.934326</td>\n",
       "      <td>-3.656014</td>\n",
       "      <td>-0.174089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013967</td>\n",
       "      <td>1.840238</td>\n",
       "      <td>1.573067</td>\n",
       "      <td>-1.189240</td>\n",
       "      <td>1.262000</td>\n",
       "      <td>0.248559</td>\n",
       "      <td>1.868664</td>\n",
       "      <td>1.695083</td>\n",
       "      <td>0.504797</td>\n",
       "      <td>1.674304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>turned</td>\n",
       "      <td>79</td>\n",
       "      <td>1.706041</td>\n",
       "      <td>0.922434</td>\n",
       "      <td>2.386130</td>\n",
       "      <td>1.205237</td>\n",
       "      <td>1.970052</td>\n",
       "      <td>2.095382</td>\n",
       "      <td>-1.556233</td>\n",
       "      <td>-1.698806</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.635460</td>\n",
       "      <td>0.741163</td>\n",
       "      <td>0.593997</td>\n",
       "      <td>0.541251</td>\n",
       "      <td>2.336679</td>\n",
       "      <td>1.149196</td>\n",
       "      <td>-0.686093</td>\n",
       "      <td>0.338912</td>\n",
       "      <td>0.658016</td>\n",
       "      <td>0.825516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>worked</td>\n",
       "      <td>79</td>\n",
       "      <td>1.471462</td>\n",
       "      <td>-2.069940</td>\n",
       "      <td>0.900406</td>\n",
       "      <td>1.020264</td>\n",
       "      <td>-0.695869</td>\n",
       "      <td>2.575898</td>\n",
       "      <td>-0.334781</td>\n",
       "      <td>0.034287</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.307653</td>\n",
       "      <td>0.215437</td>\n",
       "      <td>0.973429</td>\n",
       "      <td>-1.771111</td>\n",
       "      <td>1.192339</td>\n",
       "      <td>1.518461</td>\n",
       "      <td>0.604623</td>\n",
       "      <td>-0.800338</td>\n",
       "      <td>1.954622</td>\n",
       "      <td>1.228762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>fell</td>\n",
       "      <td>79</td>\n",
       "      <td>2.250671</td>\n",
       "      <td>-0.579359</td>\n",
       "      <td>1.779205</td>\n",
       "      <td>-0.511911</td>\n",
       "      <td>1.767967</td>\n",
       "      <td>0.359483</td>\n",
       "      <td>0.418023</td>\n",
       "      <td>-0.580308</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.265788</td>\n",
       "      <td>-0.026169</td>\n",
       "      <td>-0.018351</td>\n",
       "      <td>-0.232168</td>\n",
       "      <td>-0.220955</td>\n",
       "      <td>-1.086520</td>\n",
       "      <td>0.354990</td>\n",
       "      <td>-1.656791</td>\n",
       "      <td>1.420098</td>\n",
       "      <td>-0.892427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>walked</td>\n",
       "      <td>79</td>\n",
       "      <td>-0.559191</td>\n",
       "      <td>-0.851606</td>\n",
       "      <td>3.278703</td>\n",
       "      <td>0.421348</td>\n",
       "      <td>0.808665</td>\n",
       "      <td>2.832968</td>\n",
       "      <td>-0.811344</td>\n",
       "      <td>0.698291</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.263208</td>\n",
       "      <td>-0.367131</td>\n",
       "      <td>0.482377</td>\n",
       "      <td>-0.319783</td>\n",
       "      <td>1.243508</td>\n",
       "      <td>0.785570</td>\n",
       "      <td>-0.167478</td>\n",
       "      <td>-0.327947</td>\n",
       "      <td>-0.387275</td>\n",
       "      <td>0.333321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>passed</td>\n",
       "      <td>79</td>\n",
       "      <td>-0.700865</td>\n",
       "      <td>-1.055217</td>\n",
       "      <td>0.388894</td>\n",
       "      <td>-0.908164</td>\n",
       "      <td>-0.227375</td>\n",
       "      <td>2.736056</td>\n",
       "      <td>-0.451057</td>\n",
       "      <td>-1.490430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.611520</td>\n",
       "      <td>0.275892</td>\n",
       "      <td>0.455183</td>\n",
       "      <td>-1.519473</td>\n",
       "      <td>0.491040</td>\n",
       "      <td>0.243957</td>\n",
       "      <td>-0.683840</td>\n",
       "      <td>-1.106611</td>\n",
       "      <td>-0.608950</td>\n",
       "      <td>0.165953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>sat</td>\n",
       "      <td>79</td>\n",
       "      <td>-0.423072</td>\n",
       "      <td>0.128984</td>\n",
       "      <td>3.753227</td>\n",
       "      <td>0.122920</td>\n",
       "      <td>0.847830</td>\n",
       "      <td>1.466750</td>\n",
       "      <td>0.965369</td>\n",
       "      <td>0.681821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.811730</td>\n",
       "      <td>0.150481</td>\n",
       "      <td>1.241813</td>\n",
       "      <td>0.130148</td>\n",
       "      <td>1.160748</td>\n",
       "      <td>-0.125668</td>\n",
       "      <td>1.168625</td>\n",
       "      <td>0.193511</td>\n",
       "      <td>0.936777</td>\n",
       "      <td>-0.559015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>dropped</td>\n",
       "      <td>79</td>\n",
       "      <td>-0.383524</td>\n",
       "      <td>-0.652033</td>\n",
       "      <td>0.908911</td>\n",
       "      <td>-0.836851</td>\n",
       "      <td>0.413005</td>\n",
       "      <td>2.628306</td>\n",
       "      <td>-0.635752</td>\n",
       "      <td>-0.655634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.758298</td>\n",
       "      <td>0.554554</td>\n",
       "      <td>1.051755</td>\n",
       "      <td>-0.838268</td>\n",
       "      <td>0.680231</td>\n",
       "      <td>1.350811</td>\n",
       "      <td>0.075676</td>\n",
       "      <td>0.366876</td>\n",
       "      <td>-0.373661</td>\n",
       "      <td>1.012633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>ran</td>\n",
       "      <td>79</td>\n",
       "      <td>0.361934</td>\n",
       "      <td>0.243449</td>\n",
       "      <td>2.031370</td>\n",
       "      <td>0.136541</td>\n",
       "      <td>0.394178</td>\n",
       "      <td>2.692471</td>\n",
       "      <td>-0.936266</td>\n",
       "      <td>-0.290775</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.391611</td>\n",
       "      <td>-0.234006</td>\n",
       "      <td>0.150626</td>\n",
       "      <td>-0.140738</td>\n",
       "      <td>0.482502</td>\n",
       "      <td>0.901058</td>\n",
       "      <td>-0.164626</td>\n",
       "      <td>-0.426220</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>-0.356614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>threw</td>\n",
       "      <td>79</td>\n",
       "      <td>0.374563</td>\n",
       "      <td>0.198650</td>\n",
       "      <td>1.053886</td>\n",
       "      <td>-0.525755</td>\n",
       "      <td>0.706801</td>\n",
       "      <td>2.039049</td>\n",
       "      <td>-0.911903</td>\n",
       "      <td>-0.319938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828911</td>\n",
       "      <td>-0.426121</td>\n",
       "      <td>0.762216</td>\n",
       "      <td>-0.072124</td>\n",
       "      <td>0.731776</td>\n",
       "      <td>0.811950</td>\n",
       "      <td>0.719629</td>\n",
       "      <td>-0.412104</td>\n",
       "      <td>-0.581550</td>\n",
       "      <td>-0.452559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>pulled</td>\n",
       "      <td>79</td>\n",
       "      <td>0.386547</td>\n",
       "      <td>-0.199603</td>\n",
       "      <td>1.840434</td>\n",
       "      <td>-0.806935</td>\n",
       "      <td>1.267950</td>\n",
       "      <td>2.283539</td>\n",
       "      <td>-0.677398</td>\n",
       "      <td>-0.292797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.004660</td>\n",
       "      <td>0.376251</td>\n",
       "      <td>0.198505</td>\n",
       "      <td>0.071961</td>\n",
       "      <td>0.734711</td>\n",
       "      <td>1.038993</td>\n",
       "      <td>-0.128734</td>\n",
       "      <td>-0.348680</td>\n",
       "      <td>0.008867</td>\n",
       "      <td>-0.054492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>drove</td>\n",
       "      <td>79</td>\n",
       "      <td>-0.901452</td>\n",
       "      <td>-0.963296</td>\n",
       "      <td>1.333272</td>\n",
       "      <td>-0.506482</td>\n",
       "      <td>-0.066964</td>\n",
       "      <td>2.260604</td>\n",
       "      <td>-0.802550</td>\n",
       "      <td>0.446602</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.160487</td>\n",
       "      <td>-0.432767</td>\n",
       "      <td>0.629038</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>1.195749</td>\n",
       "      <td>0.194307</td>\n",
       "      <td>0.148141</td>\n",
       "      <td>-0.057069</td>\n",
       "      <td>0.520690</td>\n",
       "      <td>0.606673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  cluster         0         1         2         3         4  \\\n",
       "108       got       79  0.053381 -1.936365  0.561727 -1.378119  0.132917   \n",
       "160      went       79  0.042044 -1.707980  1.142836 -0.004915  0.386709   \n",
       "257      left       79 -1.456957 -1.699758  1.478573 -1.753642 -0.371985   \n",
       "267     found       79  2.265918 -0.968295 -1.647641  0.484722 -1.522750   \n",
       "277      came       79 -0.463607 -2.136055  1.442872  0.372457  0.354493   \n",
       "354      took       79  1.114481 -0.710591 -0.304834 -1.296521  0.667023   \n",
       "423     comes       79 -1.403478 -1.069754  0.811411  1.503335  0.009120   \n",
       "435      goes       79  0.390273 -0.666014  0.277956  1.113418  0.499086   \n",
       "553    turned       79  1.706041  0.922434  2.386130  1.205237  1.970052   \n",
       "610    worked       79  1.471462 -2.069940  0.900406  1.020264 -0.695869   \n",
       "816      fell       79  2.250671 -0.579359  1.779205 -0.511911  1.767967   \n",
       "993    walked       79 -0.559191 -0.851606  3.278703  0.421348  0.808665   \n",
       "1023   passed       79 -0.700865 -1.055217  0.388894 -0.908164 -0.227375   \n",
       "1068      sat       79 -0.423072  0.128984  3.753227  0.122920  0.847830   \n",
       "1206  dropped       79 -0.383524 -0.652033  0.908911 -0.836851  0.413005   \n",
       "1404      ran       79  0.361934  0.243449  2.031370  0.136541  0.394178   \n",
       "1446    threw       79  0.374563  0.198650  1.053886 -0.525755  0.706801   \n",
       "1551   pulled       79  0.386547 -0.199603  1.840434 -0.806935  1.267950   \n",
       "1556    drove       79 -0.901452 -0.963296  1.333272 -0.506482 -0.066964   \n",
       "\n",
       "             5         6         7    ...           90        91        92  \\\n",
       "108   5.273778 -0.025414  0.231150    ...    -2.206438 -2.622548  0.020733   \n",
       "160   3.767817 -1.365376  3.451374    ...    -0.635770 -0.976915  3.296836   \n",
       "257   2.490468 -0.260113  1.041301    ...    -1.268819  1.306178  0.626147   \n",
       "267   4.678810  0.581851 -1.220346    ...    -0.228626 -0.747722  1.109239   \n",
       "277   4.310445  0.089965  1.507177    ...    -0.663894 -0.929912  1.818840   \n",
       "354   2.175258 -1.702568  1.774679    ...     0.351849 -0.873941  2.864876   \n",
       "423   2.599219 -0.858298 -0.852993    ...    -0.244975  0.126007 -0.506222   \n",
       "435   2.934326 -3.656014 -0.174089    ...     0.013967  1.840238  1.573067   \n",
       "553   2.095382 -1.556233 -1.698806    ...    -1.635460  0.741163  0.593997   \n",
       "610   2.575898 -0.334781  0.034287    ...    -1.307653  0.215437  0.973429   \n",
       "816   0.359483  0.418023 -0.580308    ...    -3.265788 -0.026169 -0.018351   \n",
       "993   2.832968 -0.811344  0.698291    ...    -1.263208 -0.367131  0.482377   \n",
       "1023  2.736056 -0.451057 -1.490430    ...    -0.611520  0.275892  0.455183   \n",
       "1068  1.466750  0.965369  0.681821    ...    -0.811730  0.150481  1.241813   \n",
       "1206  2.628306 -0.635752 -0.655634    ...    -0.758298  0.554554  1.051755   \n",
       "1404  2.692471 -0.936266 -0.290775    ...    -1.391611 -0.234006  0.150626   \n",
       "1446  2.039049 -0.911903 -0.319938    ...    -0.828911 -0.426121  0.762216   \n",
       "1551  2.283539 -0.677398 -0.292797    ...    -1.004660  0.376251  0.198505   \n",
       "1556  2.260604 -0.802550  0.446602    ...    -1.160487 -0.432767  0.629038   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "108  -0.913719  0.762811 -2.355414 -0.921598 -2.747892  0.625829  1.551328  \n",
       "160  -0.683126  1.684834  1.090186 -0.055105 -0.434413  0.496267  0.416987  \n",
       "257  -1.218425  0.605352  0.320120  0.584418 -0.003564 -0.069379 -0.068601  \n",
       "267  -1.819456  0.687684  0.321745 -2.132119 -1.349787 -1.379100  0.301172  \n",
       "277  -0.628678  0.912052 -0.169193 -0.065466 -2.231793 -0.408411  1.179361  \n",
       "354   0.223393  1.311705  0.827495  0.233130 -2.768774  0.957672 -1.084769  \n",
       "423  -0.422775  0.281179  0.217244  1.608713  0.263210  0.448479  2.090702  \n",
       "435  -1.189240  1.262000  0.248559  1.868664  1.695083  0.504797  1.674304  \n",
       "553   0.541251  2.336679  1.149196 -0.686093  0.338912  0.658016  0.825516  \n",
       "610  -1.771111  1.192339  1.518461  0.604623 -0.800338  1.954622  1.228762  \n",
       "816  -0.232168 -0.220955 -1.086520  0.354990 -1.656791  1.420098 -0.892427  \n",
       "993  -0.319783  1.243508  0.785570 -0.167478 -0.327947 -0.387275  0.333321  \n",
       "1023 -1.519473  0.491040  0.243957 -0.683840 -1.106611 -0.608950  0.165953  \n",
       "1068  0.130148  1.160748 -0.125668  1.168625  0.193511  0.936777 -0.559015  \n",
       "1206 -0.838268  0.680231  1.350811  0.075676  0.366876 -0.373661  1.012633  \n",
       "1404 -0.140738  0.482502  0.901058 -0.164626 -0.426220  0.001151 -0.356614  \n",
       "1446 -0.072124  0.731776  0.811950  0.719629 -0.412104 -0.581550 -0.452559  \n",
       "1551  0.071961  0.734711  1.038993 -0.128734 -0.348680  0.008867 -0.054492  \n",
       "1556  0.027959  1.195749  0.194307  0.148141 -0.057069  0.520690  0.606673  \n",
       "\n",
       "[19 rows x 102 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['word']=='ran']\n",
    "\n",
    "result[(result['cluster']==79) & (~result['word'].isin(['example','excuse','opportunity','option','incident']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.609285714285715"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)/1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cand_vocab = np.array(w2vModel.wv.index2word)\n",
    "synset_vocab = [wn.synsets(word) for word in cand_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('iodine.n.01'),\n",
       "  Synset('one.n.01'),\n",
       "  Synset('i.n.03'),\n",
       "  Synset('one.s.01')],\n",
       " []]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_vocab[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_cluster_id = 0\n",
    "syn_cluster = [None] * len(synset_vocab)\n",
    "syn_set_list = []\n",
    "for i, syns_x in enumerate(synset_vocab):\n",
    "    if syns_x == []: continue\n",
    "    syns_x = set(syns_x)\n",
    "    overlapped_dict = {}\n",
    "    for j, syn_set in enumerate(syn_set_list):\n",
    "        intsec = syns_x.intersection(syn_set)\n",
    "        if intsec:\n",
    "            overlapped_dict[j] = len(intsec)\n",
    "    if len(overlapped_dict) == 0:\n",
    "        syn_set_list.append(syns_x)\n",
    "        syn_cluster[i] = len(syn_set_list)-1\n",
    "        continue\n",
    "    sorted_intsec = sorted(overlapped_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_set_id = sorted_intsec[0][0]\n",
    "    syn_set_list[top_set_id].update(syns_x) \n",
    "    syn_cluster[i] = top_set_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now that we have the synonym clusters, we put the names back\n",
    "df_syn_olp_cluster = pd.DataFrame(syn_cluster, index=cand_vocab)\n",
    "df_n = pd.DataFrame({'cluster': df_syn_olp_cluster[0], 'word': df_syn_olp_cluster.index})\n",
    "df_n = df_n[df_n.cluster >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completeness_score': 0.4268017890729324,\n",
       " 'homogeneity_score': 0.16331547703777072}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import homogeneity_score, completeness_score\n",
    "import pandas as pd\n",
    "\n",
    "def eval_syn_cluster(df_word2cluster, df_syn_cluster):\n",
    "    val_vocab = list(df_syn_cluster[df_syn_cluster.cluster >= 0].index)\n",
    "    y_pred = list(df_word2cluster[df_word2cluster.word.isin(val_vocab)].cluster)\n",
    "    y_true = list(df_syn_cluster[df_syn_cluster.cluster >= 0].cluster.apply(int))\n",
    "    return {'homogeneity_score':homogeneity_score(labels_pred=y_pred, labels_true=y_true),\n",
    "            'completeness_score':completeness_score(labels_pred=y_pred, labels_true=y_true)}\n",
    "\n",
    "eval_syn_cluster(result,df_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-35af3cb2eecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mNUM_CLUSTERS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_CLUSTERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2vModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 n_jobs=self.n_jobs)\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, return_n_iter)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single\u001b[0;34m(X, n_clusters, x_squared_norms, max_iter, init, verbose, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    445\u001b[0m             _labels_inertia(X, x_squared_norms, centers,\n\u001b[1;32m    446\u001b[0m                             \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                             distances=distances)\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[1;31m# computation of the means is also called the M-step of EM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_labels_inertia\u001b[0;34m(X, x_squared_norms, centers, precompute_distances, distances)\u001b[0m\n\u001b[1;32m    577\u001b[0m         inertia = _k_means._assign_labels_array(\n\u001b[1;32m    578\u001b[0m             X, x_squared_norms, centers, labels, distances=distances)\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "cand_vocab = np.array(w2vModel.wv.index2word)\n",
    "synset_vocab = [wn.synsets(word) for word in cand_vocab]\n",
    "last_cluster_id = 0\n",
    "syn_cluster = [None] * len(synset_vocab)\n",
    "syn_set_list = []\n",
    "for i, syns_x in enumerate(synset_vocab):\n",
    "    if syns_x == []: continue\n",
    "    syns_x = set(syns_x)\n",
    "    overlapped_dict = {}\n",
    "    for j, syn_set in enumerate(syn_set_list):\n",
    "        intsec = syns_x.intersection(syn_set)\n",
    "        if intsec:\n",
    "            overlapped_dict[j] = len(intsec)\n",
    "    if len(overlapped_dict) == 0:\n",
    "        syn_set_list.append(syns_x)\n",
    "        syn_cluster[i] = len(syn_set_list)-1\n",
    "        continue\n",
    "    sorted_intsec = sorted(overlapped_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_set_id = sorted_intsec[0][0]\n",
    "    syn_set_list[top_set_id].update(syns_x) \n",
    "    syn_cluster[i] = top_set_id\n",
    "df_n = pd.DataFrame({'cluster': df_syn_olp_cluster[0], 'word': df_syn_olp_cluster.index})\n",
    "df_n = df_n[df_n.cluster >= 0]\n",
    "    \n",
    "    \n",
    "final_vals = pd.read_csv('cluster_eval_sysnet.csv')\n",
    "init_val = int(final_vals.clusters.max() - 1)\n",
    "kfold = 2000\n",
    "for k in range(init_val,kfold):\n",
    "    X = w2vModel[w2vModel.wv.index2word]\n",
    "    NUM_CLUSTERS = k+2\n",
    "    kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    words = pd.DataFrame(list(w2vModel.wv.index2word),columns=['word'])\n",
    "    clusters = pd.DataFrame(list(labels),columns=['cluster'])\n",
    "    features = pd.DataFrame(w2vModel.wv.vectors)\n",
    "    result = words.merge(clusters,left_index=True,right_index=True)\n",
    "    result = result.merge(features,left_index=True,right_index=True)\n",
    "\n",
    "    score = eval_syn_cluster(result,df_n)\n",
    "    score['clusters'] = NUM_CLUSTERS\n",
    "    final_vals = final_vals.append(score,ignore_index=True)\n",
    "    final_vals.to_csv('cluster_eval_sysnet.csv',encoding='utf-8',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
