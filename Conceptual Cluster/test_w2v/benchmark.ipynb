{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b991415eed17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGlove\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from glove import Corpus, Glove\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from tabulate import tabulate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_glove(data, LEARNING_RATE=0.05, EPOCHS=5, NO_THREADS=4, EMBEDDING_DIM=100):\n",
    "    model = None\n",
    "    corpus = Corpus()\n",
    "    corpus.fit(data, window=10)\n",
    "    model = Glove(no_components=EMBEDDING_DIM,learning_rate=LEARNING_RATE)\n",
    "    model.fit(corpus.matrix, epochs=EPOCHS,no_threads=NO_THREADS,verbose=True)\n",
    "    model.add_dictionary(corpus.dictionary)\n",
    "    return model\n",
    "\n",
    "def create_word2vec(data,EMBEDDING_DIM=100):\n",
    "    model = word2vec.Word2Vec(data, size=EMBEDDING_DIM)\n",
    "    return model\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "    \n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "    \n",
    "def preprocess_text(posts):\n",
    "    text = str(posts['post_title'])+'. '+ str(posts['post_text'])\n",
    "    text =  re.sub('tl[;]?dr','',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+[0-9]+[s]?[ /\\(,)]*f[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+[0-9]+[s]?[ /\\(,)]*m[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+f[ /\\(,)]*[0-9]+[s]?[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+m[ /\\(,)]*[0-9]+[s]?[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[0-9]+','NUM',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('u/[^\\s]+','AT_USER',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',text,flags=re.IGNORECASE)  #Convert www.* or https?://* to <url>\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*[eE]dit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*EDIT\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*big edit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*important edit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*[uU]pdate\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*UPDATE\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*big update\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*important update\")[0]\n",
    "    text = text.split(\"[.]?\\nfor an update\")[0]\n",
    "    text = text.replace('\\r', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepare sentences\n",
    "c_train = pd.read_csv('data/c_train2.csv')\n",
    "c_test = pd.read_csv('data/c_test2.csv')\n",
    "c_data = pd.concat([c_train,c_test],sort=False)\n",
    "c_data = c_data.loc[:,['post_created_utc', 'full_link', 'post_id', 'post_num_comments',\n",
    "       'post_score', 'subreddit', 'post_title', 'post_text']]\n",
    "\n",
    "nc_train = pd.read_csv('data/nc_train2.csv')\n",
    "nc_test = pd.read_csv('data/nc_test2.csv')\n",
    "nc_data = pd.concat([nc_train,nc_test],sort=False)\n",
    "nc_data = nc_data.loc[:,['post_created_utc', 'full_link', 'post_id', 'post_num_comments',\n",
    "       'post_score', 'subreddit', 'post_title', 'post_text']]\n",
    "\n",
    "full_data = pd.concat([c_data,nc_data],sort=False)\n",
    "full_data = full_data.sample(len(full_data))\n",
    "posts = full_data.apply(preprocess_text,axis=1)\n",
    "data_sentences = []\n",
    "for post in posts:\n",
    "    sent_tokenize_list = sent_tokenize(post)\n",
    "    data = [nltk.word_tokenize(sentence) for sentence in sent_tokenize_list]\n",
    "    data_sentences = data_sentences + data \n",
    "len(data_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define training set with label\n",
    "c_data['class'] = 'c'\n",
    "nc_data['class'] = 'nc'\n",
    "train = pd.concat([c_data,nc_data])\n",
    "train = train.sample(len(train)) #to shuffle\n",
    "posts = train.apply(preprocess_text,axis=1)\n",
    "X = []\n",
    "for post in posts:\n",
    "    data = [word for word in nltk.word_tokenize(post)]\n",
    "    X.append(data)\n",
    "y = train['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(len(X),len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a85cff4dd14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start training...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgloveModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_glove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mw2vModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m#is ok to train the model with the full dataset as we are not providing labels.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2vModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2vModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_glove' is not defined"
     ]
    }
   ],
   "source": [
    "print('start training...')\n",
    "gloveModel = create_glove(data_sentences)\n",
    "w2vModel = create_word2vec(data_sentences)\n",
    "#is ok to train the model with the full dataset as we are not providing labels.\n",
    "w2v = {w: vec for w, vec in zip(w2vModel.wv.index2word, w2vModel.wv.syn0)}\n",
    "glove = {w: vec for w, vec in zip(gloveModel.dictionary, gloveModel.word_vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "etree_glove = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(glove)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_glove_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(glove)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with the classics - naive bayes of the multinomial and bernoulli varieties\n",
    "# with either pure counts or tfidf features\n",
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "scores = pd.read_csv('scores.csv')\n",
    "\n",
    "all_models = [\n",
    "    (\"w2v\", etree_w2v),\n",
    "    (\"w2v_tfidf\", etree_w2v_tfidf),\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "#     (\"svc\", svc),\n",
    "#     (\"svc_tfidf\", svc_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"glove\", etree_glove),\n",
    "    (\"glove_tfidf\", etree_glove_tfidf)\n",
    "]\n",
    "\n",
    "kfold = 10\n",
    "# scores = pd.DataFrame()\n",
    "for name,model in all_models:\n",
    "    if len(scores[(scores['model']==name) & (scores['kfold']==kfold)])>0:\n",
    "        continue\n",
    "    result = cross_val_score(model,X,y,cv=kfold).mean()\n",
    "    scores = scores.append({'model':name,'score':result,'kfold':kfold}, ignore_index=True)\n",
    "    scores.to_csv('scores.csv',encoding='utf-8',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2149282432880231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\envs\\gensim\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#clustering w2v\n",
    "print (w2vModel.similarity('this', 'is'))\n",
    "# w2v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
