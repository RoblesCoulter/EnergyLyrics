{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "# from glove import Corpus, Glove\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "# from sklearn.svm import SVC\n",
    "# import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "# from collections import Counter, defaultdict\n",
    "# from tabulate import tabulate\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_glove(data, LEARNING_RATE=0.05, EPOCHS=5, NO_THREADS=4, EMBEDDING_DIM=100):\n",
    "    model = None\n",
    "    corpus = Corpus()\n",
    "    corpus.fit(data, window=10)\n",
    "    model = Glove(no_components=EMBEDDING_DIM,learning_rate=LEARNING_RATE)\n",
    "    model.fit(corpus.matrix, epochs=EPOCHS,no_threads=NO_THREADS,verbose=True)\n",
    "    model.add_dictionary(corpus.dictionary)\n",
    "    return model\n",
    "\n",
    "def create_word2vec(data,EMBEDDING_DIM=100):\n",
    "    model = word2vec.Word2Vec(data, size=EMBEDDING_DIM)\n",
    "    return model\n",
    "\n",
    "def preprocess_text(posts):\n",
    "    text = str(posts['post_title'])+'. '+ str(posts['post_text'])\n",
    "    text =  re.sub('tl[;]?dr','',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+[0-9]+[s]?[ /\\(,)]*f[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+[0-9]+[s]?[ /\\(,)]*m[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+f[ /\\(,)]*[0-9]+[s]?[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[ \\(\\[]+m[ /\\(,)]*[0-9]+[s]?[ \\]\\)]+',' ',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('[0-9]+','NUM',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('u/[^\\s]+','AT_USER',text,flags=re.IGNORECASE)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',text,flags=re.IGNORECASE)  #Convert www.* or https?://* to <url>\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*[eE]dit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*EDIT\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*big edit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*important edit\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*[uU]pdate\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*UPDATE\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*big update\")[0]\n",
    "    text = text.split(\"[.]?\\n[\\* \\[\\(/]*important update\")[0]\n",
    "    text = text.split(\"[.]?\\nfor an update\")[0]\n",
    "    text = text.replace('\\r', '')\n",
    "    return text\n",
    "\n",
    "#calculate two things,\n",
    "#specificness how good is the cluster and the elements similar to each other/ how well can other elements be described by their neighbors\n",
    "#uniqueness is how differnet this cluster to others\n",
    "def cluster_score(clean_cluster,c_syn,k):\n",
    "    unique_clusters = clean_cluster.loc[:,['cluster']]\n",
    "    unique_clusters = unique_clusters.drop_duplicates()\n",
    "    tot_spec = 0\n",
    "    tot_uni = 0\n",
    "    for num,c in unique_clusters.iterrows():\n",
    "        union = clean_cluster[clean_cluster['cluster']==c['cluster']].merge(c_syn[c_syn['cluster']==c['cluster']],how='inner',left_on=['word'],right_on=['syn'])\n",
    "        specificness = len(union)/len(clean_cluster[clean_cluster['cluster']==c['cluster']])\n",
    "        union = clean_cluster[clean_cluster['cluster']!=c['cluster']].merge(c_syn[c_syn['cluster']==c['cluster']],how='inner',left_on=['word'],right_on=['syn'])\n",
    "        uniqueness = 1 - (len(union)/(len(clean_cluster)-len(clean_cluster[clean_cluster['cluster']==c['cluster']])))\n",
    "        tot_spec = tot_spec + specificness\n",
    "        tot_uni = tot_uni + uniqueness\n",
    "    tot_spec = tot_spec/len(unique_clusters)\n",
    "    tot_uni = tot_uni/len(unique_clusters)\n",
    "    return {'spec':tot_spec,'uni':tot_uni,'k':k}\n",
    "\n",
    "    \n",
    "def generate_syn_info(cluster):\n",
    "    cluster_syn = pd.DataFrame()\n",
    "    unique_clusters = cluster.loc[:,['cluster']]\n",
    "    unique_clusters = unique_clusters.drop_duplicates()\n",
    "    for cnum,c in unique_clusters.iterrows():\n",
    "#         print('starting cluster...',c[0])\n",
    "        cur_cluster = cluster[cluster['cluster']==c['cluster']]\n",
    "        syns = []\n",
    "        for wnum,word in cur_cluster.iterrows():\n",
    "            for s in wn.synsets(word['word']):\n",
    "                syn = s.name().split('.')[0]\n",
    "                if syn.find('_')<0:  #filter out composed words\n",
    "                    syns.append(syn)\n",
    "            #syns.append(word['word'])\n",
    "        this_cluster = pd.DataFrame(syns,columns=['syn'])\n",
    "        this_cluster['cluster'] = c[0]\n",
    "        this_cluster = this_cluster.drop_duplicates()\n",
    "        cluster_syn = pd.concat([cluster_syn,this_cluster])         \n",
    "    return cluster_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359557"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare sentences\n",
    "c_train = pd.read_csv('data/c_train2.csv')\n",
    "c_test = pd.read_csv('data/c_test2.csv')\n",
    "c_data = pd.concat([c_train,c_test],sort=False)\n",
    "c_data = c_data.loc[:,['post_created_utc', 'full_link', 'post_id', 'post_num_comments',\n",
    "       'post_score', 'subreddit', 'post_title', 'post_text']]\n",
    "\n",
    "nc_train = pd.read_csv('data/nc_train2.csv')\n",
    "nc_test = pd.read_csv('data/nc_test2.csv')\n",
    "nc_data = pd.concat([nc_train,nc_test],sort=False)\n",
    "nc_data = nc_data.loc[:,['post_created_utc', 'full_link', 'post_id', 'post_num_comments',\n",
    "       'post_score', 'subreddit', 'post_title', 'post_text']]\n",
    "\n",
    "full_data = pd.concat([c_data,nc_data],sort=False)\n",
    "full_data = full_data.sample(len(full_data))\n",
    "posts = full_data.apply(preprocess_text,axis=1)\n",
    "data_sentences = []\n",
    "for post in posts:\n",
    "    sent_tokenize_list = sent_tokenize(post)\n",
    "    data = [nltk.word_tokenize(sentence) for sentence in sent_tokenize_list]\n",
    "    data_sentences = data_sentences + data \n",
    "len(data_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "print('start training...')\n",
    "# gloveModel = create_glove(data_sentences)\n",
    "w2vModel = create_word2vec(data_sentences)\n",
    "#is ok to train the model with the full dataset as we are not providing labels.\n",
    "w2v = {w: vec for w, vec in zip(w2vModel.wv.index2word, w2vModel.wv.syn0)}\n",
    "# glove = {w: vec for w, vec in zip(gloveModel.dictionary, gloveModel.word_vectors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21456987528821345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "#clustering w2v\n",
    "print (w2vModel.similarity('this', 'is'))\n",
    "# w2vModel.wv.index2word\n",
    "# print (w2vModel.most_similar(positive=['hello'], negative=[], topn=10))\n",
    "\n",
    "# w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from nltk.cluster import KMeansClusterer\n",
    "# import nltk\n",
    "# X = w2vModel[w2vModel.wv.index2word]\n",
    "# NUM_CLUSTERS=10\n",
    "# kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "# assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "# # print (assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words = pd.DataFrame(list(w2vModel.wv.index2word),columns=['word'])\n",
    "# clusters = pd.DataFrame(list(assigned_clusters),columns=['cluster'])\n",
    "# features = pd.DataFrame(w2vModel.wv.syn0)\n",
    "\n",
    "# result = words.merge(clusters,left_index=True,right_index=True)\n",
    "# result = result.merge(features,left_index=True,right_index=True)\n",
    "# result.to_csv('nltk_clusters10.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-394-9b388844e7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mNUM_CLUSTERS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_CLUSTERS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2vModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 n_jobs=self.n_jobs)\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, return_n_iter)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single\u001b[0;34m(X, n_clusters, x_squared_norms, max_iter, init, verbose, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    445\u001b[0m             _labels_inertia(X, x_squared_norms, centers,\n\u001b[1;32m    446\u001b[0m                             \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                             distances=distances)\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[1;31m# computation of the means is also called the M-step of EM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_labels_inertia\u001b[0;34m(X, x_squared_norms, centers, precompute_distances, distances)\u001b[0m\n\u001b[1;32m    577\u001b[0m         inertia = _k_means._assign_labels_array(\n\u001b[1;32m    578\u001b[0m             X, x_squared_norms, centers, labels, distances=distances)\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "\n",
    "final_vals = pd.read_csv('cluster_eval.csv')\n",
    "init_val = int(final_vals.k.max() - 1)\n",
    "kfold = 2000\n",
    "for i in range(init_val,kfold):\n",
    "    X = w2vModel[w2vModel.wv.index2word]\n",
    "    NUM_CLUSTERS=i+2\n",
    "    kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "    kmeans.fit(X)\n",
    "    labels = kmeans.labels_\n",
    "    words = pd.DataFrame(list(w2vModel.wv.index2word),columns=['word'])\n",
    "    clusters = pd.DataFrame(list(labels),columns=['cluster'])\n",
    "    features = pd.DataFrame(w2vModel.wv.vectors)\n",
    "    result = words.merge(clusters,left_index=True,right_index=True)\n",
    "    result = result.merge(features,left_index=True,right_index=True)\n",
    "    \n",
    "    clean_cluster = result\n",
    "    clean_cluster = clean_cluster[clean_cluster['word']==clean_cluster['word']]\n",
    "\n",
    "    for num,c in result.iterrows():\n",
    "        try:\n",
    "            w1 = wn.synsets(c['word'])\n",
    "            if len(w1)==0:\n",
    "                clean_cluster = clean_cluster[clean_cluster['word']!=c['word']]\n",
    "        except:\n",
    "            clean_cluster = clean_cluster[clean_cluster['word']!=c['word']]\n",
    "    clean_cluster['word'] = clean_cluster.apply(lambda row: row['word'].lower(),axis=1)\n",
    "    clean_cluster = clean_cluster.loc[:,['word','cluster']]\n",
    "    clean_cluster = clean_cluster.drop_duplicates()\n",
    "    \n",
    "    c_syn = generate_syn_info(clean_cluster)\n",
    "    score = cluster_score(clean_cluster,c_syn,NUM_CLUSTERS)\n",
    "    final_vals = final_vals.append(score,ignore_index=True)\n",
    "    final_vals.to_csv('cluster_eval.csv',encoding='utf-8',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Richie\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "X = w2vModel[w2vModel.wv.index2word]\n",
    "NUM_CLUSTERS=500\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n",
    "words = pd.DataFrame(list(w2vModel.wv.index2word),columns=['word'])\n",
    "clusters = pd.DataFrame(list(labels),columns=['cluster'])\n",
    "features = pd.DataFrame(w2vModel.wv.vectors)\n",
    "result = words.merge(clusters,left_index=True,right_index=True)\n",
    "result = result.merge(features,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.cluster.value_counts()\n",
    "result.to_csv('500_cluster.csv',encoding='utf-8',index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
